{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "title: 'House-Cleaning: Getting rid of outliers'\n",
    "author: 'Corrie'\n",
    "date: '2020-02-15'\n",
    "slug: outlier-handling\n",
    "categories: \n",
    "    - Python\n",
    "tags: \n",
    "    - Python\n",
    "    - Outlier Detection\n",
    "comments: yes\n",
    "image: images/tea_with_books.jpg\n",
    "menu: ''\n",
    "share: yes\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "knitr::opts_chunk$set(echo=F)\n",
    "library(reticulate)\n",
    "library(tidyverse)\n",
    "use_condaenv(\"blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with real-world data presents many challenges that sanitized data from text book simply don't have. One of them is how to handle outlier. Outliers are defined as points that differ significantly from other data points and they are especially common in data obtained through manual input.  <br>\n",
    "For example, on an online listing site, someone might have accidently pressed the zero-key a bit too often and suddenly the small appartment for sale is as expensive a palace. While it is obvious in this example that the recorded data does not represent the real world, other outliers can be more tricky. What is for example with the old villa in small town offered for 1€? It might be one of these run-down houses in a dying town and thus could be the actual offering price or a simple input mistake.\n",
    "\n",
    "Very roughly we can say that there are two categories of outliers:\n",
    "\n",
    "- Outliers that don't represent a real observation, but instead result from an error or mistake. I'll call them \"proper outlier\".\n",
    "- Outliers that do represent a real observation, but look extremely different compared with other data. I'll call them \"improper outlier\".\n",
    "\n",
    "Deciding which type of outlier one is dealing with can be difficult and one usually needs to check the context. Ideally, you'll only want to discard \"proper outliers\" and keep the improper ones. However, both types of outlier can be problematic for any subsequent analysis or modelling. In this post, I will mostly focus on identifying outliers in general and won't talk too much about distinguishing between the two. Nevertheless, I highly recommend you to always check what type of outlier you're dealing with and to determine the cause of the outlier.\n",
    "But now let's talk about how we can identify outliers.\n",
    "\n",
    "A year ago, I scraped some [online rental offers](https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany) and except for throwing away some duplicates that arose probably due to the way I scraped them, this is still very much the raw data and thus still has a bunch of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "blue = \"#008fd5\"\n",
    "red = \"#fc4f30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.style.use('corrie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"/home/corrie/Documents/Projects/immoscout/data/immo_data_feb_2020.csv\")\n",
    "d[\"totalRent\"] = np.where((d[\"totalRent\"].isnull()) | (d[\"totalRent\"] == 0),\n",
    "                          np.where( d[\"serviceCharge\"].notnull(), d[\"baseRent\"] + d[\"serviceCharge\"], d[\"baseRent\"]),\n",
    "                          d[\"totalRent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to see if there are any outliers in your data is to just plot it. Let's look at the total rent versus the living space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=80)\n",
    "ax.scatter(d[\"livingSpace\"],d[\"totalRent\"], s=6, alpha=0.6)\n",
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€]\")\n",
    "ax.set_title(\"Rent vs Living Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this plot remind you of all the times you wanted to have a quick look at your data, see if there's any interesting pattern or some insight to be gained just to then be reminded of the fact that your data is still dirty? It definitely does for me. There are some flats or houses that are larger than 10,000 sqm which, according to [the Measure of Things](https://www.bluebulbprojects.com/MeasureOfThings/results.php?comp=area&unit=m2&amt=10000&sort=pr&p=1) is twice as big as Bill Gate's home. And that place with a rent of 10 million? Doesn't look right.\n",
    "\n",
    "One way to get a nicer plot, is to just eyeball the axes, use whatever knowledge you have about rents and flats and pick some threshold above or below which you discard the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "too_large = d[\"livingSpace\"] > 2000\n",
    "too_expensive = d[\"totalRent\"] > 2e5\n",
    "alrightish = ~too_large & ~too_expensive\n",
    "fig, ax = plt.subplots(dpi=120)\n",
    "ax.scatter(d[\"livingSpace\"][alrightish],d[\"totalRent\"][alrightish], s=6, alpha=0.6)\n",
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€]\")\n",
    "ax.set_title(\"Rent vs Living Space\\nNot too large nor too expensive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks better than before but still not great. One could iteratively try smaller thresholds until the blob in the bottom left covers most of the plot area. I think this can be a valid way to obtain reasonable thresholds but might not be very feasible if you have more variables and it's also not quite satisfying if you like to automate stuff. <br>\n",
    "In this blog post, I want to describe two simple methods to identify outliers.\n",
    "\n",
    "The first one is the interquartile range rule and might be familiar to you if you had some classes in statistics.\n",
    "\n",
    "## Interquartile Range Rule\n",
    "[Quartiles](https://en.wikipedia.org/wiki/Quartile) are important summary statistics of any continuous data. There are three quartiles, $Q_1$, $Q_2$ and $Q_3$, which are the 25th, 50th (the median) and 75th percentiles. The interquartile range $IQR$ is then the difference between the first and third quartile: \n",
    "$IQR = Q_3 - Q_1$\n",
    "The interquartile range thus covers half of the data which is also why they're used for boxplots: the boxy part is exactly the interquartile range. \n",
    "\n",
    "A good rule of thumb is to say that every point above $Q_3 +1.5 IQR$ or below $Q_1 - 1.5 IQR$ is an outlier.\n",
    "In python, we can compute this as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "def iqr(data):\n",
    "    \"\"\"compute the interquartile range (excluding nan)\"\"\"\n",
    "    return np.nanquantile(data, 0.75) - np.nanquantile(data, 0.25)\n",
    "\n",
    "def iqr_rule(data, factor=1.5):\n",
    "    \"\"\"returns an outlier filter mask using the iqr rule\"\"\"\n",
    "    iqr_ = iqr(data)\n",
    "    upper_fence = np.nanquantile(data, 0.75) + factor*iqr_\n",
    "    lower_fence = np.nanquantile(data, 0.25) - factor*iqr_\n",
    "    return (data <= upper_fence) & (data >= lower_fence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, the same plot as above, after applying the IQR rule to both our variables then looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "alrightish = iqr_rule(d[\"livingSpace\"]) & iqr_rule(d[\"totalRent\"])\n",
    "fig, ax = plt.subplots(dpi=120)\n",
    "ax.scatter(d[\"livingSpace\"][alrightish],d[\"totalRent\"][alrightish], s=6, alpha=0.6)\n",
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€]\")\n",
    "ax.set_title(\"Rent vs Living Space\\nIQR rule\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The blob now covers the whole plot area and we get a much better view of the largest chunk of the data. <br>\n",
    "However, there is a hard, rectangular edge. It looks like we cut off the data too generously and a considereable part of the data is now hidden from view because they're either larger than 140sqm or more expensive than 1750€ per month. And while I personally wouldn't rent a flat this expensive, it still sounds like very realistic rents and sizes. If I would have a flat share with three or four friends, 2000€ seems like a reasonable rent in Berlin.\n",
    "\n",
    "To adjust this rule of thumb, we can increase the factor with which we multiply the IQR. Instead of 1.5, we can use e.g. 2.5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_rule_25 = lambda x: iqr_rule(x, factor=2.5)\n",
    "alrightish = iqr_rule_25(d[\"livingSpace\"]) & iqr_rule_25(d[\"totalRent\"])\n",
    "g = sns.jointplot(x=d[\"livingSpace\"][alrightish],y=d[\"totalRent\"][alrightish],\n",
    "              s=6, alpha=0.6, height=7, marginal_kws=dict(bins=100) )\n",
    "g = g.ax_joint.set(xlabel=\"Living Space [sqm]\", \n",
    "                    ylabel=\"Total Rent [€]\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle(\"Adjusted IQR Rule\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot has less cut-off than before but there's still a rectangular border.  <br>\n",
    "I also added the histograms for each margin to get a look at the individual distributions. One can see that the distribution for the rent does not follow a normal distribution but is highly right-skewed. The distribution for the living space also seems to be slightly right-skewed. <br>\n",
    "Furthermore, we know that both rents and living space should only have values strictly above zero. (There are actually a few observations that have a value of zero for the living space or the total rent. This is clearly not realistic.) So it is probably more reasonable to model both variables assuming a log-normal distribution. <br>\n",
    "I manually assigned values of 0.5 to the observations that have a value of zero. Seems somehow reasonable to say that there's not much difference between the rent being 0€ or 50ct.\n",
    "\n",
    "Let's look at the whole data again but this time with log-transformed rent and living space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "d[\"livingSpace_m\"] =  np.where(d[\"livingSpace\"] <= 0, 0.5, d[\"livingSpace\"])\n",
    "d[\"totalRent_m\"] = np.where(d[\"totalRent\"] <= 0, 0.5, d[\"totalRent\"])\n",
    "logbins=np.logspace(0,np.log(10e3),500)\n",
    "g = sns.jointplot(x=d[\"livingSpace_m\"],y=d[\"totalRent_m\"], \n",
    "                  s=6, alpha=0.6, height=7, marginal_kws=dict(bins=logbins)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ax_joint.set_yscale(\"log\")\n",
    "g.ax_joint.set_xscale(\"log\")\n",
    "g.ax_joint.set_xlabel(\"Living Space [sqm]\")\n",
    "g.ax_joint.set_ylabel(\"Total Rent [€]\")\n",
    "g.ax_marg_x.set_xscale(\"log\")\n",
    "g.ax_marg_y.set_xscale(\"log\")\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle(\"Log-Transformed Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the same plot as our first one, just in log-scale. So only transforming the data already brought a big improvement.\n",
    "\n",
    "Now using the IQR rule on the transformed data, we get the following result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "d[\"logRent\"] = np.log(d[\"totalRent_m\"])\n",
    "d[\"logSpace\"] = np.log(d[\"livingSpace_m\"])\n",
    "\n",
    "fig, ax  = plt.subplots(dpi=120)\n",
    "alrightish = iqr_rule_25(d[\"logSpace\"]) & iqr_rule_25(d[\"logRent\"])\n",
    "d[\"outlier\"] = np.where(alrightish, \"no_outlier\", \"outlier\")\n",
    "\n",
    "scatter = ax.scatter(d[\"livingSpace\"][alrightish],d[\"totalRent\"][alrightish], s=6, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€\")\n",
    "ax.set_title(\"IQR rule on transformed data\\nWithout Outliers\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "This is the data after removing the so identified outliers. The rectangular borders don't seem as bad as before but they're still there.  <br>\n",
    "Especially in the bottom left, there's a hard cut throwing away flats that are too small. The smallest place that is not considered an outlier has a size of 16.97sqm. As often with thresholds, it begs the question why a flat of size 16.97sqm is okay but many flats of size 16.6sqm are not. <br>\n",
    "One problem here is that the data likely also contains many shared flats. The size in the offer would then be the size of the room but the rent would be relatively high for such a small living area since one also pays for common areas in a shared flat.\n",
    "\n",
    "To better understand which points where flagged as outliers, let's also plot all points again with outliers highlighted in red. To be able to see all points at once, I use log-scale again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(dpi=120)\n",
    "sc = ax.scatter(x=d[d.outlier == \"outlier\"].livingSpace_m, c=red, alpha=0.6,\n",
    "              y=d[d.outlier == \"outlier\"].totalRent_m, label=\"Outlier\", s=6)\n",
    "\n",
    "sc = ax.scatter(x=d[d.outlier == \"no_outlier\"].livingSpace_m, c=blue, alpha=0.6,\n",
    "              y=d[d.outlier == \"no_outlier\"].totalRent_m, label=\"Not Outlier\", s=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€]\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_title(\"IQR rule on transformed data\\n on log-scale\")\n",
    "leg = ax.legend(markerscale=3)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, around 1500 data points were classified as outliers. This sounds like a lot but is actually less than 1% of the total data.\n",
    "We can now see that we not only lose data in the lower left corner but there's also a hard corner in the upper right. One could increase the factor used in the IQR rule to get rid of the hard corners but this would still not lead to optimal results.\n",
    "One problem with the IQR rule is that we apply it separately on each variable. That is, the cut-off window will always be rectangular. In this example though, the two variables are highly correlated and thus a rectangular window is not a good fit. Better would be to use an oval cut-off window.\n",
    "Such an oval window can for example be obtained by using a multivariate Gaussian distribution.\n",
    "\n",
    "## Gaussian Mixtures for Outlier Detection\n",
    "So how does this work? We first fit a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) on our data and then compute the likelihood for each data point according to this distribution. All data points with a very low likelihood, much lower than the other data points, are then classified as outliers. Let's visualize this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# thanks joseph https://joseph-long.com/writing/colorbars/\n",
    "def colorbar(mappable, label):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    last_axes = plt.gca()\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n",
    "    cbar = fig.colorbar(mappable, cax=cax, label=label)\n",
    "    plt.sca(last_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklego.mixture import BayesianGMMOutlierDetector\n",
    "\n",
    "np.random.seed(20)\n",
    "exam = np.random.multivariate_normal([-10, 2], [[1.8, 1.7], [1.7, 2.3]], (1000,))\n",
    "outlier = np.array([[-7, -1], [-8, 7.5]])\n",
    "exam = np.vstack([exam, outlier])\n",
    "mod = BayesianGMMOutlierDetector(n_components=1, threshold=3.5, method=\"stddev\").fit(exam)\n",
    "\n",
    "df_ex= pd.DataFrame({\"x1\": exam[:, 0], \"x2\": exam[:, 1],\n",
    "                   \"loglik\": -mod.score_samples(exam), \n",
    "                   \"prediction\": mod.predict(exam).astype(str)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=80)\n",
    "norm = mpl.colors.Normalize(vmin=df_ex.loglik.min(),vmax=df_ex.loglik.max())\n",
    "scatter = ax.scatter(df_ex.x1, df_ex.x2, \n",
    "                          c=df_ex.loglik, cmap=\"viridis\", norm=norm,\n",
    "                          s=6, label=\"log-likelihood\")\n",
    "colorbar(scatter, label=\"log-likelihood\")\n",
    "ax.scatter(outlier[:,0], outlier[:,1], \n",
    "                c=df_ex.loglik[1000:] ,cmap=\"viridis\", norm=norm,\n",
    "                s=35)\n",
    "ax.set_title(\"Multivariate Normal Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled points from a multivariate normal with a high correlation between the $x$ and $y$ variable, similarly as in our data. The points are colored by their log-likelihood, a darker color means a lower log-likelihood. I added two outliers by hand, plotted slightly larger, one in the lower right and one in the upper middle-right. As you can see, they have a much lower log-likelihood compared to the other points. We can use this and classify all points with a very low likelihood as outliers. We can for example say that all points that are more than 3.5 standard deviations away from the mean log-likelihood of all points are outliers. Let's see how this would compare to the IQR rule:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1.5\n",
    "x1_max = np.nanquantile(exam[:,0], 0.75) + factor*iqr(exam[:,0])\n",
    "x1_min = np.nanquantile(exam[:,0], 0.25) - factor*iqr(exam[:,0])\n",
    "\n",
    "x2_max = np.nanquantile(exam[:,1], 0.75) + factor*iqr(exam[:,1])\n",
    "x2_min = np.nanquantile(exam[:,1], 0.25) - factor*iqr(exam[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(dpi=80)\n",
    "ax.scatter(df_ex.x1[df_ex.prediction == \"-1\"], df_ex.x2[df_ex.prediction == \"-1\"], c=red, label=\"low likelihood\", s=35)\n",
    "ax.scatter(df_ex.x1[df_ex.prediction == \"1\"], df_ex.x2[df_ex.prediction == \"1\"], c=blue, s=6)\n",
    "ax.axvline(x=x1_max, c=\"grey\", label=\"IQR rule\", alpha=0.3, linewidth=5)\n",
    "ax.axvline(x=x1_min, c=\"grey\", alpha=0.3, linewidth=5)\n",
    "ax.axhline(y=x2_max, c=\"grey\", alpha=0.3, linewidth=5)\n",
    "ax.axhline(y=x2_min, c=\"grey\", alpha=0.3, linewidth=5)\n",
    "ax.set_title(\"IQR rule vs Multivariate Outlier Detector\")\n",
    "ax.legend(markerscale=1.5, bbox_to_anchor=(0.81, .45))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same points where now all points with a low likelihood, according to the rule specified above, are colored in red. The grey lines give the rectangular threshold as obtained from the IQR rule (using a factor of 1.5). There are quite a few points at the lower left that would be classified as outlier by the IQR rule but whose likelihood is mostly still above the log-likelihood threshold we picked. In the upper right corner, the two methods mostly agree on what classifies as an outlier but the IQR rule misses the one manually added outlier in the lower right corner. Obviously, I added this particular outlier because it highlights how a multivariate outlier detector can find outliers that arise from a correlation between two variables. Neither the $x$ nor the $y$ value of this  point is very unusual, only the combination makes it an outlier. \n",
    "\n",
    "Fortunately, there's a package that implements this method: [scikit-lego](https://github.com/koaning/scikit-lego). The package follows the scikit-learn API and adds some additional classifiers (such as the Gaussian mixture classifier and outlier detector) but also useful transformers and a pipeline debugger.\n",
    "I'm going to use the function `GMMOutlierDetector` which implements the whole procedure described above: it fits a multivariate Gaussian on our data, computes the likelihood for each point and points with a low likelihood are flagged as outlier. Here, GMM stands for Gaussian Mixture Model.\n",
    "Vincent, one of the developer of scikit-lego, explains the whole method in a few more sentences in his [talk](https://www.youtube.com/watch?v=aICqoAG5BXQ) at the PyData Berlin 2019.\n",
    "\n",
    "Let's apply this to our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklego.mixture import GMMOutlierDetector\n",
    "\n",
    "num_cols = [\"logRent\", \"logSpace\"]\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(),\n",
    "                     GMMOutlierDetector(threshold=1.5, method=\"stddev\") )\n",
    "\n",
    "pipe = pipe.fit(d[num_cols])\n",
    "outlier = pipe.predict(d[num_cols])\n",
    "\n",
    "d[\"outlier\"] = np.where(outlier == -1, \"outlier\", \"no_outlier\")\n",
    "\n",
    "d.outlier.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian mixture outlier dectetor classified slightly more points as outlier than the IQR rule. In percentage, this is still less than 1% though.\n",
    "Let's visualize which points were classified as outliers: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=120)\n",
    "max_space = d[d.outlier == \"no_outlier\"].livingSpace.max()\n",
    "max_rent = d[d.outlier == \"no_outlier\"].totalRent.max()\n",
    "sc = ax.scatter(x=d[d.outlier == \"no_outlier\"].livingSpace_m, c=blue, alpha=0.6,\n",
    "              y=d[d.outlier == \"no_outlier\"].totalRent_m, label=\"Not Outlier\", s=6)\n",
    "sc = ax.scatter(x=d[d.outlier == \"outlier\"].livingSpace_m, c=red, alpha=0.6,\n",
    "              y=d[d.outlier == \"outlier\"].totalRent_m, label=\"Outlier\", s=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, right = ax.set_xlim(-40, max_space + 200)\n",
    "bottom, top = ax.set_ylim(-500, max_rent + 800)\n",
    "ax.set_title(\"Using GMM for Outlier Detection\")\n",
    "leg = ax.legend(markerscale=3)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the IQR rule, the method reliably detects all flats way too large or too expensive as outliers. The difference is indeed in the threshold window. Especially in the upper right we can see that the cut-off window is now elliptic instead of rectangular. \n",
    "\n",
    "Also good to see that points with a living space above 200sqm with rents below 1000€ are classified as outliers. That seems reasonable for me. Same for flats with a living space less than 80sqm with a rent above 2000€ or 3000€. Also seems reasonable.\n",
    "\n",
    "However, in the lower left we see again a rather hard threshold going through our blob of data points. That doesn't look much better to what we had before. Indeed, if we look at some outliers from the lower left, we find some (depressing) realistic examples, such as this tiny flat in one of the most popular areas of Munich, newly renovated with high quality furniture for a total rent of 900€. Depressingly expensive but not unrealistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "ex1 = d[[\"totalRent\", \"livingSpace\", \"description\", \"regio2\"]][d.outlier == \"outlier\"].iloc[0]\n",
    "print(ex1[[\"totalRent\", \"livingSpace\", \"regio2\"]]); ex1.description[0:545]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures enhanced\n",
    "\n",
    "Personally, I prefer to throw away as little as possible. So I would like to keep points such as the one above in my data set.  \n",
    "Also, imagine we would want to analyse afterwards how rent prices developed in Munich. Throwing out these examples would make Munich look cheaper than it actually is and might heavily bias our results.  \n",
    "One option would be to now play around with the threshold until you get a result you like. I often found this to be very difficult and the results not necessarily very satisfying.  \n",
    "One problem I also encountered is that some outliers that are really very far out there (think e.g. a rent of 120,000€), these extreme outlier points influenced the fitting algorithm so that the fitted multivariate normal distribution was very wide. This then would mean that quite a few proper outliers would easily be missed when increasing the threshold.\n",
    "\n",
    "So I came up with the following method:\n",
    "What if instead of fitting on the whole data set, we only fit the outlier detector on a small subset. After all, outliers are by definition rare and when we fit on a small sample there is a high probability that it doesn't contain outliers which then makes it easier to detect outliers.   However, you might easily be unlucky with the random sample you got, either then finding way too many or too little outliers. Thus, instead of just sampling and fitting once, I repeatedly sample and fit. Each time fitting on a small sample, predicting on the whole data, and then taking the mean of how often a point was classified as an outlier. This way, I also get a probability how likely a point is to be an outlier! Neat!\n",
    "\n",
    "I experimented a bit which settings go best and I found that for a data set as big as this one, fitting on 1% of the data gives good results. For the number of iterations, I'm using 50 here but found 30 to 40 iterations to also work fine. I have found 30 iterations to be the lowest number of iterations that still gives relatively stable results. If you use lower number of iterations, you'll might end up with a very different number of outliers each time you run it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "def prob_outlier(outlier_detector_pipe, data, iterations=50, p=0.01):\n",
    "    \"\"\"repeatedly performs outlier detection on samples of the data set\n",
    "    and then computes the relative frequency of how often a point was \n",
    "    classified as an outlier.\"\"\"\n",
    "    sample_size = int(len(data) * p)\n",
    "\n",
    "    outlier_ar = np.empty((0, len(data)) )\n",
    "    for i in range(iterations):\n",
    "        outlier_detector_pipe.fit(data.sample(sample_size))\n",
    "\n",
    "        outlier_ar = np.append(outlier_ar,  [outlier_detector_pipe.predict(data)], axis=0)\n",
    "\n",
    "    outlier = (outlier_ar == -1).mean(axis=0)\n",
    "    return outlier\n",
    "\n",
    "num_cols = [\"logRent\", \"logSpace\"]\n",
    "np.random.seed(20)\n",
    "d[\"outlier\"] = prob_outlier(pipe, d[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thus obtaining outlier probabilities, we can have a short look at the distribution of the outlier probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(d[\"outlier\"], log=True, bins=30, ec=\"darkblue\")\n",
    "plt.title(\"Distribution over outlier probabilities\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Counts (in log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most points are never classified as outliers, makes sense, most points should not be outlier. There is a small number of points that always get classified as outliers, these are the points where we can be very sure that they're outliers.  \n",
    "I will use a rather conservative threshold and declare everything above 0.97 as outlier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "np.sum(d[\"outlier\"] > 0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, with the IQR rule we identified around 1500 outliers and with the Gaussian mixture around 1700.   \n",
    "We reduced the number of outliers by more than half! If you care about throwing away as little as possible, this is great! And even if you want to throw away more, it is very easy to change the threshold to be less conservative.\n",
    "\n",
    "Let's have a look at the points we detect as outliers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(dpi=120)\n",
    "d[\"outlier_pred\"] = np.where(d[\"outlier\"] > 0.97, \"outlier\", \"no_outlier\")\n",
    "max_space = d[d.outlier_pred == \"no_outlier\"].livingSpace.max()\n",
    "max_rent = d[d.outlier_pred == \"no_outlier\"].totalRent.max()\n",
    "\n",
    "sc = ax.scatter(x=d[d.outlier_pred == \"no_outlier\"].livingSpace_m, c=blue, alpha=0.6,\n",
    "              y=d[d.outlier_pred == \"no_outlier\"].totalRent_m, label=\"Not Outlier\", s=6)\n",
    "sc = ax.scatter(x=d[d.outlier_pred == \"outlier\"].livingSpace_m, c=red, alpha=0.6,\n",
    "              y=d[d.outlier_pred == \"outlier\"].totalRent_m, label=\"Outlier\", s=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_title(\"Using enhanced GMM for Outlier Detection\")\n",
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Total Rent [€]\")\n",
    "left, right = ax.set_xlim(-40, max_space + 100)\n",
    "bottom, top = ax.set_ylim(-500, max_rent + 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still remove all the super far out outliers and all flats where either the living space or total rent is very close to zero but the method removes much less of the very small but expensive flats (that are probably either in expensive places like Munich or are shared flats).\n",
    "We can have a look at the descriptions of some of the outliers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "temp = d[[\"totalRent\", \"livingSpace\",\"regio2\", \"description\", ]][d.outlier_pred == \"outlier\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": false,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "py$temp %>%\n",
    "    as_tibble() %>%\n",
    "    mutate(description = substr(description, 1, 75)) %>%\n",
    "    knitr::kable(\"html\") %>%\n",
    "    kableExtra::kable_styling(bootstrap_options = \"striped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a few objects for which it seems someone just forgot to enter the correct living space. Quite a few others are boarding houses and short term rentals. The lorem ipsum flat for more than 4000€ has also correctly been identified as outlier. There's one penthouse in Munich for 20,000€ where I'm not sure if it might be the real total rent, I'm not really familiar with rental prices of penthouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going beyond two dimensions\n",
    "A nice thing about the Gaussian mixture outlier detection method is, that it can easily be extended to more than two columns. In this data set for example there are two more variables that also commonly have input errors: the number of rooms and the construction year.\n",
    "For the construction year, we have different options to use it in our model: either use as is or use the log transformed age of a building. Unfortunately, both ways have disadvantages: If we use the construction year as is, we will detect many very old houses as outliers and even though buildings from the middle age are rare, Germany has quite a few cities with many very old buildings. If instead we use the log transformed age, we miss many outliers: there are for example suspiciously many buildings constructed in 1111. For these kind of outliers, we would need a different approach. \n",
    "For this analysis, I used the log transformed age and also log transformed the number of rooms. The later helps in identifying cases where the number of rooms is too high for the amount of living space. As a high number of observations also do not have a construction year, I will do this part only on a subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": true
   },
   "outputs": [],
   "source": [
    "d[\"age\"] = 2021.5 - d[\"yearConstructed\"]\n",
    "d[\"logAge\"] = np.log(d[\"age\"])\n",
    "d[\"logRooms\"] = np.log(d[\"noRooms\"])\n",
    "\n",
    "mask = d[\"logAge\"].notnull()\n",
    "ds = d[mask].copy()\n",
    "\n",
    "ds[\"outlier\"] = np.nan\n",
    "ds[\"outlier\"] = prob_outlier(pipe, ds[[\"logRent\", \"logSpace\", \"logRooms\", \"logAge\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's have a look at a few examples identified as outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": false,
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "temp2 = ds[ds.outlier >= 0.8][[\"totalRent\", \"yearConstructed\", \"livingSpace\", \"regio2\", \"noRooms\", \"description\"]]\\\n",
    "    .sort_values(by=\"noRooms\", ascending=False)\\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "echo": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "py$temp2 %>%\n",
    "    as_tibble() %>%\n",
    "    mutate(description = substr(description, 1, 75)) %>%\n",
    "    knitr::kable(\"html\") %>%\n",
    "    kableExtra::kable_styling(bootstrap_options = \"striped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are quite a few cases where whole appartment blocks are sold where the living space and total rent then often denotes the living space and total rent of a single unit but the number of rooms denote the number of appartment units that are up for rent. \n",
    "\n",
    "Let's have a short look at the plot for living space versus number of rooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(dpi=120)\n",
    "ds[\"outlier_pred\"] = np.where(ds[\"outlier\"] >= 0.8, \"outlier\", \"no_outlier\")\n",
    "max_space = ds[(ds.outlier_pred == \"no_outlier\") ].livingSpace.max()\n",
    "max_rooms = ds[(ds.outlier_pred == \"no_outlier\") ].noRooms.max()\n",
    "ax.scatter(x=ds[ds.outlier_pred == \"no_outlier\"].livingSpace_m, c=blue, alpha=0.6,\n",
    "              y=ds[ds.outlier_pred == \"no_outlier\"].noRooms, label=\"Not Outlier\", s=6)\n",
    "ax.scatter(x=ds[ds.outlier_pred == \"outlier\"].livingSpace_m, c=red, alpha=0.6,\n",
    "              y=ds[ds.outlier_pred == \"outlier\"].noRooms, label=\"Outlier\", s=6)\n",
    "\n",
    "left, right = ax.set_xlim(-40, max_space + 200)\n",
    "bottom, top = ax.set_ylim(-1, max_rooms + 2)\n",
    "ax.set_xlabel(\"Living Space [sqm]\")\n",
    "ax.set_ylabel(\"Number of Rooms\")\n",
    "ax.set_title(\"Enhanced Outlier Detection\\n Number of Rooms vs Living Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method identifies everything with more than 15 rooms as outlier and cases with a a very small living space (around less than 30sqm) with too many rooms as outliers. Great! It also thinks that flats with a very large living area above e.g. 100sqm but with only one room are likely outliers. That sounds very reasonable to  me. \n",
    "\n",
    "## Summary\n",
    "\n",
    "In general, outlier detection is a hard problem: what constitutes an outlier is often not very obvious and can depend on the context. I found it useful to check a few cases by hand and see if I can identify an underlying cause.  \n",
    "\n",
    "Sometimes, we can use our own domain knowledge and then identify these problematic cases in a more efficient way. For example, in this data set there are many shared flats with misleading values for the living space. A simple regex or more sophisticated text analysis on the description might be more efficient to identify these cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience, results improve significantly if variables are transformed appropriately. Both the Gaussian mixture method and the IQR rule assume that our data follows a normal distribution and results are suboptimal if our data does not. \n",
    "\n",
    "Of course, there are other outlier detection methods that don't assume normality but they always assume something and it is important to be aware of assumptions and make sure they are met. \n",
    "\n",
    "The assumptions then also determine what kind of outliers we can detect. As the methods used in this analysis assume normality and define outliers as points that are outside the center, it won't detect outliers such as flats built in 1111. There might be real flats build in 1111 and there definitely are quite a few real offers in this data built aroud the 10th century but most flats built in 1111 in this data set are probably not that old.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,echo,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
