---
title: Scraping the web or how to find a flat
author: Corrie
date: '2018-10-03'
categories:
  - R
tags:
  - R
  - scraping
slug: scraping-the-web-or-how-to-find-a-flat
comments: yes
image: images/tea_with_books.jpg
share: yes
---



<p>Berlin is a great city that used to have the reputation of affordable rents. While for sure there are still other cities much more expensive, the rents in Berlin have risen considerably. Or so says everyone of my friends and my colleagues and so does it feel looking at renting listings. But let’s try to put some data to these feelings and maybe find out in the mean time if there’s still a secret neighborhood in Berlin resisting the raising prices and who knows, maybe the data can even tell us if the neighborhood Wedding is indeed “coming”.</p>
<p>For this, I will scrape the web page <a href="https://www.immobilienscout24.de">Immoscout</a> for rental flats. It is the biggest platform in Germany, offering not just rental housing but also real estate. In this article, I will concentrate on the rental offers, I don’t yet feel like buying a house.</p>
<pre class="r"><code>url &lt;- &quot;https://www.immobilienscout24.de/Suche/S-2/Wohnung-Miete&quot;</code></pre>
<p>For scraping, we will use the library <code>rvest</code>.</p>
<pre class="r"><code>library(tidyverse)
library(rvest)     # for scraping
library(stringr)   # for string manipulation
library(glue)      # evaluate expressions in strings
library(jsonlite)  # to handle json data</code></pre>
<p>Now if we look at the URL, then it has a list of offers and at the bottom, we can select the page via drop down menu or go to the previous or next page. Our task will now be to iterate over all pages and get all listings from each page. For this, we will need to look at some HTML and CSS code. The easiest way to do is, is to have a look at the source code in your browser (if you use Chrome, just press <code>F12</code>). If you then hover over the elements, you can see the corresponding source code. Since we want to know, how many pages of listings there are, we have a look at the drop down menu at the bottom: <img src="/post/2018-10-03-web_scraper_files/immo_screenshot_crop.png" alt="image of the immoscout source code" /></p>
<p>If you know a bit of HTML, then you know that <code>&lt;div&gt;</code> represents an HTML node for a division or section. Other HTML nodes are for example</p>
<ul>
<li><code>&lt;h1&gt;</code>, <code>&lt;h2&gt;</code> etc for headings</li>
<li><code>&lt;p&gt;</code> are paragraph elements</li>
<li><code>&lt;ul&gt;</code> is an unordered list (such as this one) where each item is denoted with <code>&lt;li&gt;</code></li>
<li><code>&lt;ol&gt;</code> is an ordered list</li>
<li><code>&lt;table&gt;</code> is a table (surprise)</li>
</ul>
<p>The usual structure in an HTML file is such, that the tag for a node embraces its content: <code>&lt;tag&gt;content&lt;/tag&gt;</code>. To access any HTML node, we can use the function <code>html_nodes()</code>. First, we load in the first page and can then access any HTML node as follow:</p>
<pre class="r"><code>first_page &lt;- read_html(url)
first_page %&gt;%
  html_nodes(&quot;div&quot;) %&gt;%
  head(3)</code></pre>
<pre><code>{xml_nodeset (3)}
[1] &lt;div class=&quot;viewport&quot;&gt;\n    \n\n\n\n\n\n\n\n\n\n&lt;div class=&quot;page-wra ...
[2] &lt;div class=&quot;page-wrapper page-wrapper--with-ads page-wrapper--no-bor ...
[3] &lt;div&gt;\n    &lt;style scoped id=&quot;topnavigation__style-element&quot; type=&quot;tex ...</code></pre>
<p>We’re not actually interested in all divisions but only the one with the ID <code>&quot;pageSelection&quot;</code>. To get this specific node, we use <code>#</code> to denote that it’s an ID. In this division, there is a select node which contains all options. We can access the nested nodes as follows:</p>
<pre class="r"><code>first_page %&gt;%
  html_nodes(&quot;#pageSelection  &gt; select &gt; option&quot;) %&gt;%
  head(3)</code></pre>
<pre><code>{xml_nodeset (3)}
[1] &lt;option selected value=&quot;1&quot;&gt;1&lt;/option&gt;\n
[2] &lt;option value=&quot;2&quot;&gt;2&lt;/option&gt;\n
[3] &lt;option value=&quot;3&quot;&gt;3&lt;/option&gt;\n</code></pre>
<p>From this, it is now very easy to obtain the last page number. With <code>html_text()</code>, we extract the values for each node as text, which gives us then a vector of page numbers. The length of this vector is then the number of pages:</p>
<pre class="r"><code>(last_page_number &lt;- first_page %&gt;%
  html_nodes(&quot;#pageSelection &gt; select &gt; option&quot;) %&gt;%
  html_text() %&gt;%
  length())</code></pre>
<pre><code>[1] 4299</code></pre>
<p>As next step, we can generate a list that contains the URLs of each page. At this website, the page number is inserted somewhere in the middle of the URL for which we will take advantage of the package glue, which evaluates expressions in curly braces in strings:</p>
<pre class="r"><code>page_url &lt;- &quot;https://www.immobilienscout24.de/Suche/S-2/P-{pages}/Wohnung-Miete&quot;
pages &lt;- 1:last_page_number
page_list &lt;- glue(page_url)</code></pre>
<p>Now that we have a list for all pages, we need to find out how to actually get the information of each listing. Going back to the source code of the website, we see that the listings are contained in an unordered list (<code>&lt;ul&gt;</code>) with the ID <code>&quot;resultListItems&quot;</code> where each listing is a node with the tag <code>&lt;li&gt;</code>.</p>
<pre class="r"><code>first_page %&gt;%
  html_nodes(&quot;#resultListItems &gt; li&quot;) %&gt;%
  head(3)</code></pre>
<pre><code>{xml_nodeset (3)}
[1] &lt;li class=&quot;result-list__top-banner result-list__first-slot backgroun ...
[2] &lt;li class=&quot;result-list__listing &quot; data-id=&quot;96801682&quot;&gt;&lt;div&gt;&lt;article d ...
[3] &lt;li class=&quot;result-list__listing &quot; data-id=&quot;87684135&quot;&gt;&lt;div&gt;&lt;article d ...</code></pre>
<p>We now see that the first item in the list is not actually a listing but a banner. The listings are all of the class <code>result-list__listing</code>. To extract only the nodes of a certain class, we give the parameter <code>.class-name</code> to <code>html_nodes()</code>. So to get only the listings, we give the parameter <code>.result-list__listing</code>:</p>
<pre class="r"><code>first_page %&gt;%
  html_nodes(&quot;.result-list__listing&quot;) %&gt;%
  head(3)</code></pre>
<pre><code>{xml_nodeset (3)}
[1] &lt;li class=&quot;result-list__listing &quot; data-id=&quot;96801682&quot;&gt;&lt;div&gt;&lt;article d ...
[2] &lt;li class=&quot;result-list__listing &quot; data-id=&quot;87684135&quot;&gt;&lt;div&gt;&lt;article d ...
[3] &lt;li class=&quot;result-list__listing &quot; data-id=&quot;107513055&quot;&gt;&lt;div&gt;&lt;article  ...</code></pre>
<p>Perfect, now we have a list of all listings.</p>
<p>We could now either just scrape the information for each listing that is displayed on the front page (such as total rent, number of rooms etc) or get the link to the listing and scrape the information from there. We will do the second option here.</p>
<p>Looking again at the source code, we find the following <code>&lt;a href=&quot;/expose/123456&quot;&gt;</code>. That means, the link is given as a relative path and has the same form for each listing. The number in the link is also given as <code>data-id</code> as attribute in the <code>&lt;li&gt;</code> node. The attributes of a node are not part of the node content (and hence are not output via <code>html_text()</code>). In <code>rvest</code>, we can get attributes using the function <code>xml_attr()</code>.</p>
<pre class="r"><code>(listing_ids &lt;- first_page %&gt;%
  html_nodes(&quot;.result-list__listing&quot;) %&gt;%
  xml_attr(&quot;data-id&quot;))</code></pre>
<pre><code> [1] &quot;96801682&quot;  &quot;87684135&quot;  &quot;107513055&quot; &quot;107325427&quot; &quot;107591254&quot;
 [6] &quot;107591220&quot; &quot;89233331&quot;  &quot;93739727&quot;  &quot;107591535&quot; &quot;107591463&quot;
[11] &quot;68890908&quot;  &quot;107591252&quot; &quot;107591640&quot; &quot;75189490&quot;  &quot;107591632&quot;
[16] &quot;107525852&quot; &quot;107591573&quot; &quot;107591201&quot; &quot;107591462&quot; &quot;45066619&quot; </code></pre>
<p>This gives us all the information we need to create a list of all listings:</p>
<pre class="r"><code>listing_url &lt;- &quot;https://www.immobilienscout24.de/expose/&quot;
listing_list &lt;- str_c(listing_url, listing_ids)</code></pre>
<p>In the next step, we need to extract the relevant information from one flat listing. Again, we need to look at the source code. Rather hidden in the code is somewhere a dictionary called <code>keyValues</code> that contains all information about the flat in a compact format. Since it is actually in some javascript code, I found it easiest to just transform the whole page via <code>html_text()</code> to a string and then extract the dictionary with some regex.</p>
<p>Let’s do this exemplary for the first listing:</p>
<pre class="r"><code>list_page &lt;- read_html(listing_list[1])
list_l &lt;- list_page %&gt;%
  html_text() %&gt;%
  str_extract(&quot;(?&lt;=keyValues = )(\\{.*?\\})&quot;) %&gt;%
  str_remove_all(&quot;obj_&quot;) %&gt;% # to save some typing later on
  str_replace_all(&quot;false&quot;, &quot;FALSE&quot;) %&gt;%
  str_replace_all(&quot;true&quot;, &quot;TRUE&quot;) %&gt;%
  fromJSON()</code></pre>
<p>If you wonder what the cryptic regex means, here is a very rough breakdown: The first group, that is the first parentheses, look for the pattern <code>&quot;keyValues = &quot;</code> and then match the group in the second parentheses. This is called a positive lookbehind. The second group then matches everything in curly braces. Since regex is something I can never quit remember for long, I usually just go to <a href="https://regex101.com/">this page</a>, copy paste the text in which I’m searching a certain pattern and then try until I find something that works. Unfortunately, the page only has a few regex flavors, but I found the python flavor to be close enough. One difference in R is, that to escape special characters, you need two backslashes instead of the usual one.</p>
<p>Back to the listings, there are two information I would like to have that is not contained in the dictionary: The text description of the object and the text about the facilities. Luckily, there are much easier to find than the dictionary:</p>
<pre class="r"><code>list_l$description &lt;- list_page %&gt;%
  html_nodes(&quot;.is24qa-objektbeschreibung&quot;) %&gt;%
  html_text()

list_l$facilities &lt;- list_page %&gt;%
  html_nodes(&quot;.is24qa-ausstattung&quot;) %&gt;%
  html_text()</code></pre>
<p>For identifiability reasons, we should also add the ID of the listing itself. This way, we can later have a look at the listing page again.</p>
<pre class="r"><code>list_l$id &lt;- listing_ids[1]</code></pre>
<p>For further analysis, we can then transform the list to a data frame:</p>
<pre class="r"><code>list_df &lt;- list_l %&gt;%
  map_if(is_empty, function(x) {
    NA
  }) %&gt;%
  as.tibble()</code></pre>
<p>Let’s wrap this up in a little function:</p>
<pre class="r"><code>get_listing_data &lt;- function(listing_url) {
  list_page &lt;- try(read_html(listing_url))
  if (any(class(list_page) == &quot;try-error&quot;)) return(NULL)
  list_l &lt;- list_page %&gt;%
    html_text() %&gt;%
    str_extract(&quot;(?&lt;=keyValues = )(\\{.*?\\})&quot;) %&gt;%
    str_remove_all(&quot;obj_&quot;) %&gt;% # to save some typing later on
    str_replace_all(&quot;false&quot;, &quot;FALSE&quot;) %&gt;%
    str_replace_all(&quot;true&quot;, &quot;TRUE&quot;) %&gt;%
    fromJSON()

  list_l$description &lt;- list_page %&gt;%
    html_nodes(&quot;.is24qa-objektbeschreibung&quot;) %&gt;%
    html_text()

  list_l$facilities &lt;- list_page %&gt;%
    html_nodes(&quot;.is24qa-ausstattung&quot;) %&gt;%
    html_text()

  # extract id back from url
  list_l$id &lt;- str_extract(listing_url, &quot;(?&lt;=/)\\d+&quot;)


  list_l %&gt;%
    map_if(is_empty, function(x) {
      NA
    }) %&gt;%
    as.tibble()
}</code></pre>
<p>The hard part is almost done. So far, we only have a single data point. We now need to piece everything together and loop over all pages and listings and scrape together the information.</p>
<pre class="r"><code>for (i in 1:last_page_number) {
  link &lt;- page_list[i]
  page &lt;- try(read_html(link))
  if (!any(class(page) == &quot;try-error&quot;)) {
    listing_ids &lt;- page %&gt;%
      html_nodes(&quot;.result-list__listing&quot;) %&gt;%
      xml_attr(&quot;data-id&quot;)
    listing_url &lt;- &quot;https://www.immobilienscout24.de/expose/&quot;
    listing_list &lt;- str_c(listing_url, listing_ids)

    map(listing_list, get_listing_data) %&gt;%
      bind_rows() %&gt;%
      write_csv(paste0(&quot;../../rohdaten/&quot;, format(Sys.time(), 
                          &quot;%Y-%m-%d-%H%M%S&quot;), &quot; page &quot;, i, &quot;.csv&quot;))
  }
}</code></pre>
<p>Now, since the process of scraping is rather slow, I used the library <code>foreach</code> to run the whole thing in parallel. This only requires a few small changes to the code above. I’m no expert on parallel computing, but apparently it is needed to first register the multicore parallel backend which is done by the package <code>doMC</code>. Note that the parallel backend is system-specific, so if you run Windows or Mac, you will need a different library. Also, since the files are generated in parallel, the time stamp is not a good file name, if you want to sort the files by page, thus I put the page identifier at the front of the file name. The parallel backend unfortunately makes it a bit difficult to get the print output. To check the process of the script, I just checked how many files were already in the folder. Even with the parallel backend, going through all pages takes quite a while (depending on your internet connection), for me it was more than a day so at some point I just decided that I had collected enough data and stopped the script.</p>
<pre class="r"><code>library(foreach)     # to run in parallel
library(doMC)        # backend to run in parallel (only for linux)
n_cores &lt;- detectCores()
registerDoMC(n_cores)

foreach(i = 1:last_page_number) %dopar% {
  # cat(paste(&quot;Page&quot;, i), stdout() )

  link &lt;- page_list[i]
  page &lt;- try(read_html(link))
  if (!any(class(page) == &quot;try-error&quot;)) {
    listing_ids &lt;- page %&gt;%
      html_nodes(&quot;.result-list__listing&quot;) %&gt;%
      xml_attr(&quot;data-id&quot;)
    listing_url &lt;- &quot;https://www.immobilienscout24.de/expose/&quot;
    listing_list &lt;- str_c(listing_url, listing_ids)

    map(listing_list, get_listing_data) %&gt;%
      bind_rows() %&gt;%
      write_csv(paste0(&quot;../rawdata/&quot;, str_pad(i, 4, pad = &quot;0&quot;), &quot;_&quot;, 
                       format(Sys.time(), &quot;%Y-%m-%d-%H%M%S&quot;), &quot;.csv&quot;))
  }
  # cat(paste(&quot;Page saved&quot;), stdout() )
  link
}</code></pre>
<p>The full <code>R</code> script can be found <a href="https://github.com/corriebar/blogdown_source/blob/master/drafts/web_scraper.R">here</a>.</p>
<p>So now, let’s see if my dream flat is somewhere hidden in the scraped data.</p>
