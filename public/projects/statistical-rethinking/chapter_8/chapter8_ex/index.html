<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
    
    
        
            <meta name="twitter:card" content="summary"/>
        
    



<meta name="twitter:title" content=""/>
<meta name="twitter:description" content=""/>
<meta name="twitter:site" content="@corrieaar"/>



  	<meta property="og:title" content=" &middot; Samples of Thoughts" />
  	<meta property="og:site_name" content="Samples of Thoughts" />
  	<meta property="og:url" content="/projects/statistical-rethinking/chapter_8/chapter8_ex/" />

    
        
            <meta property="og:image" content="/images/tea_with_books.jpg"/>
        
    
    
    <meta property="og:description" content="" />
  	<meta property="og:type" content="article" />
    <meta property="article:published_time" content="0001-01-01T00:00:00Z" />

    
    

    <title> &middot; Samples of Thoughts</title>

    
    <meta name="description" content="Chapter 8 - Exercises Corrie September 11, 2018
Chapter 8 - Exercises Easy. 8E1. Which of the following is a requirement of the simple Metropolis algorithm?
 Th" />
    

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/images/favicon.ico">
	  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="/css/nav.css" />

    
    
    


<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/googlecode.min.css' rel='stylesheet' type='text/css' />



  
     
      
          <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Samples of Thoughts" />
      
      
    
    <meta name="generator" content="Hugo 0.55.5" />

    <link rel="canonical" href="/projects/statistical-rethinking/chapter_8/chapter8_ex/" />

    
      
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name":  null 
    },
    "author": {
        "@type": "Person",
        "name":  null ,
        
        "url":  null ,
        "sameAs": [
            
            
             
             
             
             
             
            
        ]
    },
    "headline": "",
    "name": "",
    "wordCount":  3772 ,
    "timeRequired": "PT18M",
    "inLanguage": {
      "@type": "Language",
      "alternateName": "en"
    },
    "url": "/projects/statistical-rethinking/chapter_8/chapter8_ex/",
    "datePublished": "0001-01-01T00:00Z",
    "dateModified": "0001-01-01T00:00Z",
    
    
    "description": "",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/projects/statistical-rethinking/chapter_8/chapter8_ex/"
    }
}
    </script>
    


    

    
<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-140745376-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>


    
    
    




<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#40485A",
          "text": "#ffffff"
        },
        "button": {
          "background": "#5B5A68",
          "text": "#ffffff"
        }
      },
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on my website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>


</head>
<body class="nav-closed">

  <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/projects">Projects</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/talks">Talks</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/about">About</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/">Home</a>
            </li>
        
        
    </ul>

    
    <a class="subscribe-button icon-feed" href="/index.xml">Subscribe</a>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">



<header class="main-header post-head no-cover">
  <nav class="main-nav clearfix">



      <ul>
        
			<li> <a class="blog-logo" href="/">Home</a> </li>
			  
			<li> <a class="blog-logo" href="/about">About</a> </li>
			  
			<li> <a class="blog-logo" href="/talks">Talks</a> </li>
			  
			<li> <a class="blog-logo" href="/projects">Projects</a> </li>
			  

            
              <a class="menu-button icon-feed" href="">&nbsp;&nbsp;Subscribe</a>
            
            
      
       </ul>
    </nav>
    
     <div class="vertical">
        <div class="main-header-content inner">
            


    <a class="bloglogo" href="https://github.com/corriebar" target="_blank">
    <span class="icon-github" style="color:white;font-size:2em"></span>
    </a>
&nbsp;









    <a class="bloglogo" href="https://twitter.com/corrieaar" target="_blank">
        <span class="icon-twitter" style="color:white;font-size:2em"></span>
    </a>
&nbsp;














            <h1 class="page-title">Samples of Thoughts</h1>
            <h2 class="page-description">about data, statistics  and everything in between</h2>
        </div>
    </div>  
    


</header>



<main class="content" role="main">




  <article class="post projects">

    <header class="post-header">
        <h1 class="post-title"></h1>
        <small></small>

        <section class="post-meta">
        
            <p class="post-reading post-line">
            <span>Estimated reading time: 18 min</span>
            </p>
        
        
        
         
        </section>
    </header>

    <section class="post-content">
      

<h1 id="chapter-8-exercises">Chapter 8 - Exercises</h1>

<p>Corrie
September 11, 2018</p>

<h1 id="chapter-8-exercises-1">Chapter 8 - Exercises</h1>

<h2 id="easy">Easy.</h2>

<p><strong>8E1.</strong> Which of the following is a requirement of the simple Metropolis algorithm?</p>

<ul>
<li>The proposal distribution must be symmetric</li>
</ul>

<p><strong>8E2.</strong> Gibbs sampling is more efficient than the Metropolis algorithm. How does it achieve this extra efficiency? Are there any limitations?</p>

<p>Gibbs sampling uses conjugate priors which allows it to make smarter proposals and is thus more efficient. The downside to this, is that it uses conjugate priors which might not be a good or valid prior from a scientific perspective. Also, it becomes quite inefficient with complex models of hundreds or more parameter.</p>

<p><strong>8E3.</strong> Which sort of parameters can Hamiltonian Monte Carlo not handle? Can you explain why?</p>

<p>HMC cannot deal with discrete parameters. The HMC kind of glides through the parameter space, where the speed depends on how quickly the density is changing. This means, it computes gradients in the parameter space. This is not possible with discrete parameters.</p>

<p><strong>8E4.</strong> Explain the difference between the effective number of samples, <code>n_eff</code> as calculated by Stan, and the actual number of samples.</p>

<p>The effective number of samples gives an estimate of the number of samples that are independent. Since Markov chains are autocorrelated, sequential samples are not independent of each other.</p>

<p><strong>8E5.</strong> Which value should <code>Rhat</code> approach, when a chain is sampling the posterior distribution correctly?</p>

<p>If a chain is sampling correctly, the <code>Rhat</code> value should approach 1. Already values slightly above 1.00, such as 1.01 can be indicative of a problem. Values of <code>Rhat</code> much higher than 1 signal a big problem. Note that even invalid chains can reach 1.00.</p>

<p><strong>8E6.</strong> Show examples of a Markov Chain that is effectively sampling from the posterior and one that is not. What about their shape indicates good or bad sampling?</p>

<p><em>Good example:</em></p>

<pre><code class="language-r">library(rethinking)
y &lt;- rnorm(100, mean=1, sd=2)
m8.1 &lt;- map2stan(
  alist(
    y ~ dnorm(mu, sigma),
    mu &lt;- alpha,
    alpha ~ dnorm(0, 10),
    sigma ~ dcauchy(0,1)
  ), 
  data=list(y=y), start=list(alpha=0, sigma=1),
  chains=2
)
</code></pre>

<pre><code>Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

Warning: Examine the pairs() plot to diagnose sampling problems
</code></pre>

<pre><code class="language-r">plot(m8.1, col=c(&quot;black&quot;, &quot;royalblue4&quot;), n_cols=1)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-1-1.png" alt="" /></p>

<p>These chains are doing good: they are stationary, that is, the mean of the chain does not go up or down but the chain stays the whole time between the values 0.5 and 1.5 for <code>alpha</code> and between 1.8 and 2.6 for <code>sigma</code>.</p>

<p><em>Bad example:</em></p>

<pre><code class="language-r">y &lt;- rnorm(100, mean=1, sd=2)
m8.2 &lt;- map2stan(
  alist(
    y ~ dnorm(mu, sigma),
    mu &lt;- a1 + a2,
    sigma ~ dcauchy(0,1)
  ), 
  data=list(y=y), start=list(a1=0, a2=0, sigma=1),
  chains=2
)
</code></pre>

<pre><code>Computing WAIC

Constructing posterior predictions
</code></pre>

<pre><code class="language-r">plot(m8.2, col=c(&quot;black&quot;, &quot;royalblue4&quot;), n_cols=1)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-2-1.png" alt="" /></p>

<p>These chains are not doing well: The chains for <code>a1</code> and <code>a2</code> go up and down and don&rsquo;t settle on a mean. While the chains for sigma are somehow closer to each other, they still didn&rsquo;t settle on a mean.</p>

<h2 id="medium">Medium.</h2>

<p><strong>8M1.</strong> Re-estimate the terrain ruggedness model from the chapter, but now using a uniform prior and an exponential prior for the standard deviation, <code>sigma</code>. The uniform prior should be <code>dunif(0, 10)</code> and the exponential should be <code>dexpo(1)</code>. Do the different priors have any detectable influence on the posterior distribution?</p>

<p>I also add the Half-Cauchy prior we used before for comparison.</p>

<pre><code class="language-r">data(rugged)
d &lt;- rugged
d$log_gdp &lt;- log(d$rgdppc_2000)
dd &lt;- d[ complete.cases(d$rgdppc_2000), ]
dd.trim &lt;- dd[ , c(&quot;log_gdp&quot;, &quot;rugged&quot;, &quot;cont_africa&quot;)]

ptm3 &lt;- proc.time()
m8.unif &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
proc.time() - ptm3

ptm4 &lt;- proc.time()
m8.exp &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
proc.time() - ptm4

ptm5 &lt;- proc.time()
m8.cauchy &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 2)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
proc.time() - ptm5
</code></pre>

<p>Comparing the outputs of the three models, the estimates of all three models are the same.</p>

<pre><code class="language-r">precis(m8.unif)
</code></pre>

<pre><code>       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a      9.23   0.14       9.03       9.46  1101    1
bR    -0.21   0.08      -0.32      -0.08   990    1
bA    -1.96   0.22      -2.29      -1.57   986    1
bAR    0.40   0.13       0.20       0.60   914    1
sigma  0.95   0.05       0.87       1.04  1366    1
</code></pre>

<pre><code class="language-r">precis(m8.exp)
</code></pre>

<pre><code>       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a      9.22   0.14       9.00       9.46  1087    1
bR    -0.20   0.08      -0.33      -0.08  1123    1
bA    -1.94   0.23      -2.28      -1.57  1031    1
bAR    0.39   0.13       0.19       0.61  1116    1
sigma  0.95   0.05       0.86       1.03  1173    1
</code></pre>

<pre><code class="language-r">precis(m8.cauchy)
</code></pre>

<pre><code>       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a      9.22   0.14       9.00       9.45   893 1.01
bR    -0.20   0.08      -0.33      -0.08   869 1.01
bA    -1.94   0.23      -2.31      -1.60   939 1.01
bAR    0.39   0.13       0.18       0.60   971 1.00
sigma  0.95   0.05       0.87       1.04  1282 1.00
</code></pre>

<p>Also comparing the trace plots doesn&rsquo;t show any discernible difference, nor do the <code>Rhat</code> values or number of effective samples differ. Comparing the <code>pairs</code> plots for each model also doesn&rsquo;t show any differences. The time needed to sample from each model is also very similar.</p>

<pre><code class="language-r">sigma.exp &lt;- extract.samples(m8.exp)$sigma
sigma.unif &lt;- extract.samples(m8.unif)$sigma
sigma.cauchy &lt;- extract.samples(m8.cauchy)$sigma

plot( density( sigma.exp, from=0.8, to=1.1, adj=1),
      lwd=1, col=&quot;royalblue4&quot;, xlab=&quot;sigma&quot;, 
      main=&quot;&quot;, ylim=c(0, 8.2))
points(density( sigma.cauchy, from=0, to=10, adj=1),
       lty=1, type=&quot;l&quot;)
points(density( sigma.unif, from=0, to=10, adj=1),
       lty=1, col=col.desat(&quot;red&quot;), type=&quot;l&quot;)
legend(&quot;topright&quot;, col=c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;)), 
       lty=c(1,1,1),legend=c(&quot;Exp&quot;, &quot;Cauchy&quot;, &quot;Unif&quot;), bty=&quot;n&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-7-1.png" alt="" /></p>

<p>Comparing the three posterior distributions for <code>sigma</code> at close scale, we can see some slight differences: the exponential prior and the Cauchy prior leads to a posterior distribution that seem to be very slightly right skewed compared to posterior by the uniform prior. However, the differences are rather small, so it is hard to say if they&rsquo;re not just by chance.</p>

<p><strong>8M2.</strong> The Cauchy and exponential prior from the model above are very weak. They can be made more informative by reducing their scale. Compare the two priors for progressively smaller values of the scaling parameter.</p>

<pre><code class="language-r">m8.exp1 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
m8.exp2 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dexp(10)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
m8.exp3 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dexp(100)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
</code></pre>

<pre><code class="language-r">m8.cauchy1 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)  
)
</code></pre>

<pre><code>Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

Warning: Examine the pairs() plot to diagnose sampling problems

Computing WAIC

Constructing posterior predictions
</code></pre>

<pre><code class="language-r">m8.cauchy2 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 0.1)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)  
)
</code></pre>

<pre><code>Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

Warning: Examine the pairs() plot to diagnose sampling problems

Computing WAIC
Constructing posterior predictions
</code></pre>

<pre><code class="language-r">m8.cauchy3 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 0.01)
  ),
  data = dd.trim, chains=2,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)  
)
</code></pre>

<pre><code>Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

Warning: Examine the pairs() plot to diagnose sampling problems

Computing WAIC
Constructing posterior predictions
</code></pre>

<p>For both the exponential and the Cauchy prior, the models are sorted from least restrictive to most restrictive. That is, <code>m8.exp3</code> and <code>m8.cauchy3</code> are the models with the most restrictive <code>sigma</code> prior.</p>

<pre><code class="language-r">coeftab(m8.exp1, m8.exp2, m8.exp3)
</code></pre>

<pre><code>      m8.exp1 m8.exp2 m8.exp3
a        9.22    9.22    9.23
bR       -0.2    -0.2    -0.2
bA      -1.94   -1.95   -1.95
bAR      0.39    0.39    0.40
sigma    0.95    0.93    0.78
nobs      170     170     170
</code></pre>

<p>The more restrictive exponential prior have a visible effect on the posterior: The estimate for sigma decreased by quite a bit and even the other parameter estimates decreased by a small amount.</p>

<pre><code class="language-r">coeftab(m8.cauchy1, m8.cauchy2, m8.cauchy3)
</code></pre>

<pre><code>      m8.cauchy1 m8.cauchy2 m8.cauchy3
a        9.22       9.22       9.22   
bR       -0.2       -0.2       -0.2   
bA      -1.95      -1.95      -1.94   
bAR      0.39       0.40       0.39   
sigma    0.95       0.94       0.95   
nobs      170        170        170   
</code></pre>

<p>Even the most restrictive Cauchy prior has not much effect on the parameter estimates. It decreases only by 0.01 which could easily also be due to chance in the sampling.</p>

<p>We can see why this is, if we compare the prior distributions:</p>

<pre><code class="language-r">par(mfrow=c(1,2))
curve(dexp(x,1), from=0, to=5, ylab=&quot;Density&quot;, xlab=&quot;sigma&quot;,
      col=&quot;royalblue4&quot;)
curve(dexp(x,10), from=0, to=5, add=T)
curve(dexp(x,100), from=0, to=5,add=T, col=col.desat(&quot;red&quot;))
mtext(&quot;Exponential Prior&quot;)
legend(&quot;topright&quot;, col=c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;)), 
       lty=c(1,1,1),legend=c(&quot;Exp(1)&quot;, &quot;Exp(10)&quot;, &quot;Exp(100)&quot;), bty=&quot;n&quot;)

curve(2*dcauchy(x, 0, 1), from=0, to=5, ylab=&quot;Density&quot;, xlab=&quot;sigma&quot;,
      col=&quot;royalblue4&quot;)
curve(2*dcauchy(x, 0, 0.1), from=0, to=5, add=T, col=&quot;black&quot;)
curve(2*dcauchy(x, 0, 0.01), from=0, to=5, add=T, col=col.desat(&quot;red&quot;))
mtext(&quot;Cauchy Prior&quot;)
legend(&quot;topright&quot;, col=c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;)), 
       lty=c(1,1,1),legend=c(&quot;Cauchy(0, 1)&quot;, &quot;Cauchy(0, 0.1)&quot;, &quot;Cauchy(0, 0.01)&quot;), bty=&quot;n&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-12-1.png" alt="" /></p>

<p>The Cauchy prior distributions have much thicker tails. While the exponential distribution very quickly concentrates and becomes very flat every else, the Cauchy distribution still places quite a bit of weight on the tails. This explains why even a rather concentrated Cauchy prior still allows for sufficient flexibility for the posterior distribution.</p>

<p>Plotting the posterior distribution for sigma supports this further:</p>

<pre><code class="language-r">sigma.exp1 &lt;- extract.samples(m8.exp1)$sigma
sigma.exp2 &lt;- extract.samples(m8.exp2)$sigma
sigma.exp3 &lt;- extract.samples(m8.exp3)$sigma

sigma.cauchy1 &lt;- extract.samples(m8.cauchy1)$sigma
sigma.cauchy2 &lt;- extract.samples(m8.cauchy2)$sigma
sigma.cauchy3 &lt;- extract.samples(m8.cauchy3)$sigma

par(mfrow=c(1,2))
plot( density( sigma.exp1, from=0.8, to=1.1, adj=1),
      lwd=1, col=&quot;royalblue4&quot;, xlab=&quot;sigma&quot;, 
      main=&quot;&quot;, ylim=c(0,8.5))
points(density( sigma.exp2, from=0, to=10, adj=1),
       lty=1, type=&quot;l&quot;)
points(density( sigma.exp3, from=0, to=10, adj=1),
       lty=1, col=col.desat(&quot;red&quot;), type=&quot;l&quot;)
legend(&quot;topright&quot;, col=c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;)), 
       lty=c(1,1,1),legend=c(&quot;Exp(1)&quot;, &quot;Exp(10)&quot;, &quot;Exp(100)&quot;), bty=&quot;n&quot;)
mtext(&quot;Exponential Prior (Posterior)&quot;)

plot( density( sigma.cauchy1, from=0.8, to=1.1, adj=1),
      lwd=1, col=&quot;royalblue4&quot;, xlab=&quot;sigma&quot;, 
      main=&quot;&quot;, ylim=c(0, 8.5))
points(density( sigma.cauchy2, from=0, to=10, adj=1),
       lty=1, type=&quot;l&quot;)
points(density( sigma.cauchy3, from=0, to=10, adj=1),
       lty=1, col=col.desat(&quot;red&quot;), type=&quot;l&quot;)
legend(&quot;topright&quot;, col=c(&quot;royalblue4&quot;, &quot;black&quot;, col.desat(&quot;red&quot;)), 
       lty=c(1,1,1),legend=c(&quot;Cauchy(0, 1)&quot;, &quot;Cauchy(0, 0.1)&quot;, &quot;Cauchy(0, 0.01)&quot;), bty=&quot;n&quot;)
mtext(&quot;Cauchy Prior (Posterior)&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-13-1.png" alt="" /></p>

<p>While the posterior of the Cauchy prior remains very robust, the exponential prior quickly lead to posterior distributions that derail towards zero. In the worst case of the prior <code>dexp(100)</code>, the posterior even goes completely off. In contrast, even further reducing the scale of the Cauchy prior to e.g. <code>dcauchy(0, 0.001)</code> does not lead to a different posterior distribution.</p>

<p><strong>8M3.</strong> Re-estimate one of the Stan models from the chapter, but at different numbers of warmup iterations. Be sure to use the same number of sampling iterations in each case. Compare the <code>n_eff</code> values. How much warmup is enough?</p>

<p>We use again the terrain ruggedness model.</p>

<pre><code class="language-r">m8.5 &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma) ,
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa,
    a ~ dnorm(0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0, 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 2)
  ), 
  data=dd.trim,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)

m8.5_1 &lt;- map2stan(m8.5, chains = 2, warmup=1, iter = 2000+1)
m8.5_5 &lt;- map2stan(m8.5, chains = 2, warmup=5, iter = 2000+5)
m8.5_10 &lt;- map2stan(m8.5, chains = 2, warmup=10, iter = 2000+10)
m8.5_20 &lt;- map2stan(m8.5, chains = 2, warmup=20, iter = 2000+20)
m8.5_30 &lt;- map2stan(m8.5, chains = 2, warmup=30, iter = 2000+30)
m8.5_40 &lt;- map2stan(m8.5, chains = 2, warmup=40, iter = 2000+40)
m8.5_50 &lt;- map2stan(m8.5, chains = 2, warmup=50, iter = 2000+50)
m8.5_100 &lt;- map2stan(m8.5, chains = 2, warmup=100, iter = 2000+100)
m8.5_500 &lt;- map2stan(m8.5, chains = 2, warmup=500, iter = 2000+500)
m8.5_1000 &lt;- map2stan(m8.5, chains = 2, warmup=1000, iter = 2000+1000)
</code></pre>

<pre><code class="language-r">l &lt;- list(m8.5_1, m8.5_5, m8.5_10,m8.5_20, m8.5_30, m8.5_40,
          m8.5_50, m8.5_100, m8.5_500, m8.5_1000)
par(mfrow=c(1,2))
v.mean &lt;- sapply(l, function(x) mean( attr(precis(x), &quot;output&quot;)$n_eff ) )
plot(c(1, 5, 10, 20, 30, 40, 50, 100, 500, 1000), 
     v.mean, type=&quot;l&quot;, log=&quot;x&quot;, 
     xlab=&quot;warmup&quot;, ylab=&quot;n_eff&quot;, col=&quot;royalblue4&quot;)
mtext(&quot;Average efficient number of samples&quot;)

r.mean &lt;- sapply(l, function(x) mean( attr(precis(x), &quot;output&quot;)$Rhat ) )
plot(c(1, 5, 10, 20, 30, 40, 50, 100, 500, 1000), 
     r.mean, type=&quot;l&quot;, log=&quot;xy&quot;, 
     xlab=&quot;warmup&quot;, ylab=&quot;Rhat&quot;, col=&quot;royalblue4&quot;)
mtext(&quot;Average Rhat&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-15-1.png" alt="" /></p>

<p>After around only 50 warmup iterations, the efficient number of samples is already close to the maximal possible. Checking the Rhat value, this one is already at 1.01 for only 10 warmup iterations.</p>

<h2 id="hard">Hard.</h2>

<p><strong>8H1.</strong> Run the model below and then inspect the posterior distribution and explain what it is accomplishing.</p>

<pre><code class="language-r">mp &lt;- map2stan(
  alist(
    a ~ dnorm(0, 1),
    b ~ dcauchy(0, 1)
  ),
  data=list(y=1),
  start=list(a=0, b=0),
  iter=1e4, warmup=100, WAIC=FALSE
)
</code></pre>

<p>The model simply samples from the two distributions: the normal and the Cauchy distribution.</p>

<pre><code class="language-r">stancode(mp)
</code></pre>

<pre><code>data{
    int&lt;lower=1&gt; N;
}
parameters{
    real a;
    real b;
}
model{
    b ~ cauchy( 0 , 1 );
    a ~ normal( 0 , 1 );
}
generated quantities{
    real dev;
    dev = 0;
}
</code></pre>

<p>The trace plots thus show samples from the two distributions:</p>

<pre><code class="language-r">plot(mp, n_cols=1, col=&quot;royalblue4&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-18-1.png" alt="" /></p>

<p>Since the Cauchy distribution has very heavy tails, every once in a while, it samples a large value which gives it this trace plot with a few spikes. Note also that the Cauchy distribution has a much smaller number of effective samples.</p>

<p>We can compare the two samples with their exact density function:</p>

<pre><code class="language-r">post &lt;- extract.samples(mp)

par(mfrow=c(1,2))
dens(post$a)
curve(dnorm(x,0,1), from=-4, to=4, add=T, lty=2)
legend(&quot;topright&quot;, lty=c(1,2), legend=c(&quot;Sample&quot;, &quot;Exact density&quot;), bty=&quot;n&quot;)
mtext(&quot;Normal&quot;)
dens(post$b,  col=&quot;royalblue4&quot;, xlim=c(-10, 10))
curve(dcauchy(x, 0, 1), from = -10, to=10, add=T, lty=2,
      col=&quot;royalblue4&quot;)
mtext(&quot;Cauchy&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-19-1.png" alt="" /></p>

<p>While the normal distribution has been approximated very well, the Cauchy distribution has been approximated less well. After all, the number of effective samples for the Cauchy distribution has been relatively small.</p>

<p><strong>8H2.</strong> Recall the divorce rate example from Chapter 5. Repeat the analysis, using map2stan this time, fitting models <code>m5.1</code>, <code>m5.2</code> and <code>m5.3</code>. Compare the models on the basis of WAIC.</p>

<pre><code class="language-r">data(&quot;WaffleDivorce&quot;)
d &lt;- WaffleDivorce
d$MedianAgeMarriage_s &lt;- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) /
  sd(d$MedianAgeMarriage)

d$Marriage_s &lt;- (d$Marriage - mean(d$Marriage))
d.trim &lt;- d[, c(&quot;Divorce&quot;, &quot;MedianAgeMarriage_s&quot;, &quot;Marriage_s&quot;)]

m5.1s &lt;- map2stan(
  alist(
    Divorce ~ dnorm( mu, sigma),
    mu &lt;- a + bA*MedianAgeMarriage_s,
    a ~ dnorm(10, 10),
    bA ~ dnorm(0, 1),
    sigma ~ dunif( 0, 10)
  ),
  data=d.trim
)
</code></pre>

<pre><code>Computing WAIC

Constructing posterior predictions
</code></pre>

<pre><code class="language-r">m5.2s &lt;- map2stan(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu &lt;- a + bR*Marriage_s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ),
  data=d.trim
)
</code></pre>

<pre><code>Computing WAIC
Constructing posterior predictions
</code></pre>

<pre><code class="language-r">m5.3s &lt;- map2stan(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu &lt;- a + bR*Marriage_s + bA*MedianAgeMarriage_s,
    a &lt;- dnorm( 10, 10),
    bR ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ),
  data=d.trim
)
</code></pre>

<pre><code>Computing WAIC
Constructing posterior predictions
</code></pre>

<pre><code class="language-r">compare(m5.1s, m5.2s, m5.3s)
</code></pre>

<pre><code>       WAIC pWAIC dWAIC weight    SE  dSE
m5.1s 186.7   4.1   0.0   0.62 12.66   NA
m5.3s 187.6   4.7   1.0   0.38 12.31 1.00
m5.2s 199.7   3.1  13.1   0.00  9.76 9.24
</code></pre>

<p>The first model, only using the predictor <code>MedianAgeMarriage_s</code>, has the lowest WAIC and most of the weight. It is closely followed by the last model, which includes both <code>MedianAgeMarriage_s</code> and <code>Marriage_s</code>, and also has about a third of the weight. The second model that only uses the predictor <code>Marriae_s</code> has a rather high WAIC and no weight, indicating that it is not a good model compared to the other two. Since the third model includes one more predictor variable, as also indicated by <code>pWAIC</code>, it performs slightly worse than the first model. After all, it adds a predictor variable that is then set to almost 0 by the model:</p>

<pre><code class="language-r">plot(coeftab(m5.1s, m5.2s, m5.3s))
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-22-1.png" alt="" /></p>

<p><strong>8H3.</strong> Sometimes changing a prior for one parameter has unanticipated effects on other parameters. This is because when a parameter is highly correlated with another parameter in the posterior, the prior influences both parameters. Take for example the leg length example from Chapter 5.</p>

<pre><code class="language-r">N &lt;- 100
height &lt;- rnorm(N, 10, 2)
leg_prop &lt;- runif(N, 0.4, 0.5)
leg_left &lt;- leg_prop*height + rnorm(N, 0, 0.02)
leg_right &lt;- leg_prop*height + rnorm(N, 0, 0.02)

d &lt;- data.frame(height, leg_left, leg_right)
</code></pre>

<p>This time, we fit the model using Stan:</p>

<pre><code class="language-r">m5.8s &lt;- map2stan(
  alist(
    height ~ dnorm( mu, sigma),
    mu &lt;- a + bl*leg_left + br*leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data=d, chains=4,
  start=list(a=10, bl=0, br=0, sigma=1)
)
</code></pre>

<p>Compare the posterior distribution of the model above to the posterior distribution produced when changing the prior for <code>br</code> so that it is strictly positive. The <code>T[0,]</code> truncates the normal distribution so that it has positive probability only above zero.</p>

<pre><code class="language-r">m5.8s2 &lt;- map2stan(
  alist(
    height ~ dnorm( mu, sigma),
    mu &lt;- a + bl*leg_left + br*leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10) &amp; T[0,],
    sigma ~ dcauchy(0, 1)
  ), 
  data=d, chains=4,
  start=list(a=10, bl=0, br=0, sigma=1)
)
</code></pre>

<p>Let&rsquo;s first have a look at the trace plots:</p>

<pre><code class="language-r">plot(m5.8s, n_cols=1, window=c(50, 2000))
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-26-1.png" alt="" /></p>

<p>The trace plot for the first model looks all good. The estimates and both <code>Rhat</code> and <code>n_eff</code> don&rsquo;t look too bad. The only thing suspicious here, is that both <code>bl</code> and <code>br</code> have the same standard deviation.</p>

<pre><code class="language-r">precis(m5.8s)
</code></pre>

<pre><code>       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a      0.35   0.37      -0.24       0.94  2321    1
bl    -1.56   2.26      -4.93       2.29  1703    1
br     3.70   2.26      -0.10       7.11  1708    1
sigma  0.64   0.05       0.57       0.71  2161    1
</code></pre>

<p>The pairs plot reveals the problematic correlation between the two parameter:</p>

<pre><code class="language-r">pairs(m5.8s)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-28-1.png" alt="" /></p>

<p>Now to the second model where the parameter <code>br</code> has a truncated prior. It had more than 1000 divergent warnings by Stan, which already does not sound good. Let&rsquo;s have a look at the trace plots:</p>

<pre><code class="language-r">plot(m5.8s2, n_col=1, window=c(50,2000))
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-29-1.png" alt="" /></p>

<p>The parameter <code>br</code> has been truncated, so it only has positive values. Now this did not only change <code>br</code> but also <code>bl</code> which now only has values below 2. While the chains otherwise still look well mixed, the number of efficient samples went down by quite a bit for the two slope parameter.</p>

<pre><code class="language-r">precis(m5.8s2)
</code></pre>

<pre><code>Warning in precis(m5.8s2): There were 456 divergent iterations during sampling.
Check the chains (trace plots, n_eff, Rhat) carefully to ensure they are valid.

       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a      0.35   0.36      -0.26       0.89  2016    1
bl    -1.86   2.03      -4.89       1.55  1001    1
br     4.00   2.03       0.67       7.12   784    1
sigma  0.64   0.05       0.57       0.72  1880    1
</code></pre>

<pre><code class="language-r">pairs(m5.8s2)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-31-1.png" alt="" /></p>

<p>Whereas before, the two parameter had both a posterior distribution close to normal, now one of them is left-skewed and the other one right-skewed. What happens is that, since both parameter correlate so strongly, we can only reliably estimate their sum. Since we force <code>br</code> to be positive, the other part of the sum, <code>bl</code>, now more often has to be negative.</p>

<p><strong>8H4.</strong> For the two models fit above, use DIC or WAIC to compare the effective number of parameters for each model. Which model has more effective parameters and why?</p>

<pre><code class="language-r">compare(m5.8s, m5.8s2)
</code></pre>

<pre><code>        WAIC pWAIC dWAIC weight    SE dSE
m5.8s2 196.2   3.2   0.0   0.55 11.53  NA
m5.8s  196.6   3.3   0.4   0.45 11.45 0.3
</code></pre>

<pre><code class="language-r">DIC(m5.8s)
</code></pre>

<pre><code>[1] 196.9839
attr(,&quot;pD&quot;)
[1] 3.795018
</code></pre>

<pre><code class="language-r">DIC(m5.8s2)
</code></pre>

<pre><code>[1] 196.5512
attr(,&quot;pD&quot;)
[1] 3.595826
</code></pre>

<p>DIC and WAIC estimate around 3 parameters for the truncated model and around 4 for the non-truncated model. Since the truncated model restricts the two parameters to either be positive or the remaining summand, it has less free parameter.</p>

<p><strong>8H5.</strong> Modify the Metropolis algorithm code from the chapter to handle the case that the island populations have a different distribution than the island labels. That is, the island&rsquo;s number will not be the same as the population.</p>

<p>We first generate random populations. I just used the same populations as before, only randomly permutated.</p>

<pre><code class="language-r">island.pop &lt;- sample(1:10, size=10, replace=FALSE)   # island population
names(island.pop) &lt;- 1:10                            # number of the island
island.pop
</code></pre>

<pre><code> 1  2  3  4  5  6  7  8  9 10 
 4  1  8  9  3  6 10  7  5  2 
</code></pre>

<pre><code class="language-r">num_weeks &lt;- 1e5
positions &lt;- rep(0, num_weeks)
current &lt;- 10              # current is still the number of the island
for (i in 1:num_weeks) {
  # record current position
  positions[i] &lt;- current
  
  # flip coin to generate proposal
  proposal &lt;- current + sample( c(-1, 1), size=1)       # proposal is now the number 
  if ( proposal &lt; 1 ) proposal &lt;- 10                    # of the proposal island
  if ( proposal &gt; 10 ) proposal &lt;- 1
  
  # move?
  # instead of taking the ratio between the island numbers
  # we now take the ratio of the island populations
  prob_move &lt;- island.pop[proposal] / island.pop[current]
  current &lt;- ifelse( runif(1) &lt; prob_move , proposal, current)
}
</code></pre>

<pre><code class="language-r">par(mfrow=c(1,2))
plot( (1:100), positions[1:100], xlab=&quot;week&quot;, ylab=&quot;island&quot;, col=&quot;royalblue4&quot;)
plot(table(positions), col=&quot;royalblue4&quot;, xlab=&quot;island&quot;, ylab=&quot;number of weeks&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-37-1.png" alt="" /></p>

<p><strong>8H6.</strong> Modify the Metropolis algorithm code from the chapter to write your own simple MCMC estimator for globe tossing data and model from Chapter 2. The model we want to fit can be specified as follow:</p>

<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%2A%7D%0Aw%20%26%5Csim%20%5Ctext%7BBinom%7D%28%20%5Ctheta%2C%20n%20%29%5C%5C%0A%5Ctheta%20%26%5Csim%20%5Ctext%7BUnif%7D%280%2C1%29%0A%5Cend%7Balign%2A%7D &quot;begin{align*}
w &amp;sim text{Binom}( theta, n" alt="\\begin{align\*}
w &amp;\\sim \\text{Binom}( \\theta, n )\\\\
\\theta &amp;\\sim \\text{Unif}(0,1)
\\end{align\*}" /><br />
\theta &amp;\sim \text{Unif}(0,1)
\end{align*}&ldquo;)</p>

<pre><code class="language-r"># the globe tossing data
w &lt;- 6
n &lt;- 9
# prior on p
p_prior &lt;- function(p) dunif(p, min=0, max=1)
# initializing MCMC
iter &lt;- 1e4
p_sample &lt;- rep(0, iter)
p_current &lt;- 0.5       # start value
for (i in 1:iter) {
  # record current p
  p_sample[i] &lt;- p_current
  
  # generate proposal
  p_proposal &lt;- runif(1, min=0, max=1)
  
  # compute likelihood for current and proposal
  lkhd_current &lt;- dbinom(w, n, p_current)
  lkhd_proposal &lt;- dbinom(w, n, p_proposal)
  
  # assuming a uniform prior of 1 over [0,1]
  # otherwise, multiply times prior at p
  
  
  # accept proposal?
  prob_accept &lt;- (lkhd_proposal *p_prior(p_proposal) ) / ( lkhd_current * p_prior(p_current) )
  p_current &lt;- ifelse( runif(1) &lt; prob_accept, p_proposal, p_current)
}
</code></pre>

<p>We can visualize the trace plot:</p>

<pre><code class="language-r">plot(p_sample, type=&quot;l&quot;, col=&quot;royalblue4&quot;)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-39-1.png" alt="" /></p>

<p>A well mixed chain.</p>

<p>We can also plot the posterior distribution:</p>

<pre><code class="language-r">dens(p_sample, col=&quot;royalblue4&quot;, adj=1)
curve(dbeta(x, w+1, n-w+1 ), from=0, to=1, add=T, lty=2)
</code></pre>

<p><img src="chapter8_Ex_files/figure-markdown_github/unnamed-chunk-40-1.png" alt="" /></p>

<p>The dashed line is the exact analytic solution. Our simple MCMC estimator doesn&rsquo;t perform too bad.</p>

    
    </section>


  <aside class="read-next">
  
      <a class="read-next-story" style="no-cover" href="/projects/statistical-rethinking/chapter_8/chapter8/">
          <section class="post">
              <h5></h5>
              
          </section>
      </a>
  
  
      <a class="read-next-story prev" style="no-cover" href="/projects/statistical-rethinking/readme/">
          <section class="post">
              <h5></h5>
          </section>
      </a>
      

</aside>



  <footer class="post-footer">


    









<section class="author">
  <h4><a href="/">Corrie</a></h4>
  
  <p>Read <a href="/">more posts</a> by this author.</p>
  
  <div class="author-meta">
    
    
  </div>
</section>




    
<section class="share">
  <h4>Share this projects</h4>
  <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=&nbsp;-&nbsp;Samples%20of%20Thoughts&amp;url=%2fprojects%2fstatistical-rethinking%2fchapter_8%2fchapter8_ex%2f"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=421');return false;">
      <span class="hidden">Twitter</span>
  </a>
  <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=%2fprojects%2fstatistical-rethinking%2fchapter_8%2fchapter8_ex%2f"
      onclick="window.open(this.href, 'facebook-share','width=580,height=551');return false;">
      <span class="hidden">Facebook</span>
  </a>
  <a class="icon-linkedin" style="font-size: 1.4em" href="https://www.linkedin.com/shareArticle?mini=true&title=&url=%2fprojects%2fstatistical-rethinking%2fchapter_8%2fchapter8_ex%2f"
               onclick="window.open(this.href, 'linkedin-share', 'width=554,height=571');return false;">
    <span class="hidden">LinkedIn</span>
    </a>

</section>




    

<div id="disqus_thread"></div>
<script>




var disqus_config = function () {
this.page.url = "\/projects\/statistical-rethinking\/chapter_8\/chapter8_ex\/";  
this.page.identifier = "\/projects\/statistical-rethinking\/chapter_8\/chapter8_ex\/"; 
};

(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://corriebar-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>








  </footer>
</article>

</main>



    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Samples of Thoughts</a> 
        &copy; Corrie Bartelheimer 2020 &middot; </section>
        
    </footer>
    </div>
    <script type="text/javascript" src="/js/jquery.js"></script>
    <script type="text/javascript" src="/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="/js/index.js"></script>
    
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>


    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML">
</script>


    
    
<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-140745376-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>


    
</body>
</html>

