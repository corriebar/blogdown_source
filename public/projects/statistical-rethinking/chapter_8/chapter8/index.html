<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    
    
        <meta name="twitter:card" content="summary"/>
    



<meta name="twitter:title" content=""/>
<meta name="twitter:description" content=""/>
<meta name="twitter:site" content="@corrieaar"/>



  	<meta property="og:title" content=" &middot; Samples of Thoughts" />
  	<meta property="og:site_name" content="Samples of Thoughts" />
  	<meta property="og:url" content="/projects/statistical-rethinking/chapter_8/chapter8/" />

    
        
            <meta property="og:image" content="/images/tea_with_books.jpg"/>
        
    
    
    <meta property="og:description" content="" />
  	<meta property="og:type" content="article" />
    <meta property="article:published_time" content="0001-01-01T00:00:00Z" />

    
    

    <title> &middot; Samples of Thoughts</title>

    
    <meta name="description" content="Markov Chain Monte Carlo Corrie September 4, 2018
8.1 King Markov and His island kingdom A simple example of the Markov Chain Monte Carlo algorithm:
num_weeks &amp;" />
    

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/images/favicon.ico">
	  <link rel="apple-touch-icon" href="/images/apple-touch-icon.png" />

    <link rel="stylesheet" type="text/css" href="/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="/css/nav.css" />

    
    
    


<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/googlecode.min.css' rel='stylesheet' type='text/css' />



  
     
      
          <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Samples of Thoughts" />
      
      
    
    <meta name="generator" content="Hugo 0.55.5" />

    <link rel="canonical" href="/projects/statistical-rethinking/chapter_8/chapter8/" />

    
      
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name":  null 
    },
    "author": {
        "@type": "Person",
        "name":  null ,
        
        "url":  null ,
        "sameAs": [
            
            
             
             
             
             
             
            
        ]
    },
    "headline": "",
    "name": "",
    "wordCount":  3933 ,
    "timeRequired": "PT19M",
    "inLanguage": {
      "@type": "Language",
      "alternateName": "en"
    },
    "url": "/projects/statistical-rethinking/chapter_8/chapter8/",
    "datePublished": "0001-01-01T00:00Z",
    "dateModified": "0001-01-01T00:00Z",
    
    
    "description": "",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/projects/statistical-rethinking/chapter_8/chapter8/"
    }
}
    </script>
    


    

    
<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-140745376-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>


    
    
    




<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js"></script>
<script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#40485A",
          "text": "#ffffff"
        },
        "button": {
          "background": "#5B5A68",
          "text": "#ffffff"
        }
      },
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on my website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://cookies.insites.com"
      }
    })});
</script>


</head>
<body class="nav-closed">

  <div class="nav">
    <h3 class="nav-title">Menu</h3>
    <a href="#" class="nav-close">
        <span class="hidden">Close</span>
    </a>
    <ul>
        
        
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/projects">Projects</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/talks">Talks</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/about">About</a>
            </li>
        
            
            <li class="nav-opened" role="presentation">
            	<a href="/">Home</a>
            </li>
        
        
    </ul>

    
    <a class="subscribe-button icon-feed" href="/index.xml">Subscribe</a>
    
</div>
<span class="nav-cover"></span>


 <div class="site-wrapper">



<header class="main-header post-head no-cover">
  <nav class="main-nav clearfix">



      <ul>
        
			<li> <a class="blog-logo" href="/">Home</a> </li>
			  
			<li> <a class="blog-logo" href="/about">About</a> </li>
			  
			<li> <a class="blog-logo" href="/talks">Talks</a> </li>
			  
			<li> <a class="blog-logo" href="/projects">Projects</a> </li>
			  

            
              <a class="menu-button icon-feed" href="">&nbsp;&nbsp;Subscribe</a>
            
            
      
       </ul>
    </nav>
    
     <div class="vertical">
        <div class="main-header-content inner">
            


    <a class="bloglogo" href="https://github.com/corriebar" target="_blank">
    <span class="icon-github" style="color:white;font-size:2em"></span>
    </a>
&nbsp;









    <a class="bloglogo" href="https://twitter.com/corrieaar" target="_blank">
        <span class="icon-twitter" style="color:white;font-size:2em"></span>
    </a>
&nbsp;














            <h1 class="page-title">Samples of Thoughts</h1>
            <h2 class="page-description">about data, statistics  and everything in between</h2>
        </div>
    </div>  
    


</header>



<main class="content" role="main">




  <article class="post projects">

    <header class="post-header">
        <h1 class="post-title"></h1>
        <small></small>

        <section class="post-meta">
        
            <p class="post-reading post-line">
            <span>Estimated reading time: 19 min</span>
            </p>
        
        
        
         
        </section>
    </header>

    <section class="post-content">
      

<h1 id="markov-chain-monte-carlo">Markov Chain Monte Carlo</h1>

<p>Corrie
September 4, 2018</p>

<h2 id="8-1-king-markov-and-his-island-kingdom">8.1 King Markov and His island kingdom</h2>

<p>A simple example of the Markov Chain Monte Carlo algorithm:</p>

<pre><code class="language-r">num_weeks &lt;- 1e5
positions &lt;- rep(0, num_weeks)
current &lt;- 10
for (i in 1:num_weeks) {
  # record current position
  positions[i] &lt;- current
  
  # flip coin to generate proposal
  proposal &lt;- current + sample( c(-1, 1), size=1)
  if ( proposal &lt; 1 ) proposal &lt;- 10
  if ( proposal &gt; 10 ) proposal &lt;- 1
  
  # move?
  prob_move &lt;- proposal / current
  current &lt;- ifelse( runif(1) &lt; prob_move , proposal, current)
}
</code></pre>

<pre><code class="language-r">par(mfrow=c(1,2))
plot( (1:100), positions[1:100], xlab=&quot;week&quot;, ylab=&quot;island&quot;, col=&quot;midnightblue&quot;)
plot(table(positions), col=&quot;midnightblue&quot;, xlab=&quot;island&quot;, ylab=&quot;number of weeks&quot;)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-2-1.png" alt="" /></p>

<h2 id="8-3-easy-hmc-map2stan">8.3 Easy HMC: <code>map2stan</code></h2>

<p>Using the terrain ruggedness data from Chapter 7:</p>

<pre><code class="language-r">library(rethinking)
data(rugged)
d &lt;- rugged
d$log_gdp &lt;- log(d$rgdppc_2000)
dd &lt;- d[ complete.cases(d$rgdppc_2000), ]
</code></pre>

<p>Fitting the old way using <code>map</code>:</p>

<pre><code class="language-r">m8.1 &lt;- map(
  alist(
    log_gdp ~ dnorm( mu, sigma ),
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa ,
    a ~ dnorm( 0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0 , 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ),
  data = dd
)
precis(m8.1)
</code></pre>

<pre><code>##        Mean StdDev  5.5% 94.5%
## a      9.22   0.14  9.00  9.44
## bR    -0.20   0.08 -0.33 -0.08
## bA    -1.95   0.22 -2.31 -1.59
## bAR    0.39   0.13  0.19  0.60
## sigma  0.93   0.05  0.85  1.01
</code></pre>

<p>To use Stan, we should do some preprocessing. In particular, preprocess all variable transformations and make a trimmed data frame, only containing the variables used in the model.</p>

<pre><code class="language-r">dd.trim &lt;- dd[ , c(&quot;log_gdp&quot;, &quot;rugged&quot;, &quot;cont_africa&quot;)]
str(dd.trim)
</code></pre>

<pre><code>## 'data.frame':    170 obs. of  3 variables:
##  $ log_gdp    : num  7.49 8.22 9.93 9.41 7.79 ...
##  $ rugged     : num  0.858 3.427 0.769 0.775 2.688 ...
##  $ cont_africa: int  1 0 0 0 0 0 0 0 0 1 ...
</code></pre>

<p>Using Stan:</p>

<pre><code class="language-r">m8.1stan &lt;- map2stan(
  alist(
    log_gdp ~ dnorm( mu, sigma) ,
    mu &lt;- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa,
    a ~ dnorm(0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0, 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 2)
  ), 
  data=dd.trim,
  start=list(a=5, bR=0, bA=0, bAR=0, sigma=1)
)
</code></pre>

<pre><code>## 
## SAMPLING FOR MODEL 'log_gdp ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 4.2e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.281279 seconds (Warm-up)
##                0.272134 seconds (Sampling)
##                0.553413 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'log_gdp ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 2.5e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.
## Adjust your expectations accordingly!
## 
## 
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## Iteration: 1 / 1 [100%]  (Sampling)
## 
##  Elapsed Time: 0 seconds (Warm-up)
##                6.1e-05 seconds (Sampling)
##                6.1e-05 seconds (Total)

## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

## Warning: Examine the pairs() plot to diagnose sampling problems

## Computing WAIC

## Constructing posterior predictions

## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]
</code></pre>

<pre><code class="language-r">precis(m8.1stan)
</code></pre>

<pre><code>##        Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
## a      9.23   0.14       9.00       9.43   416    1
## bR    -0.21   0.08      -0.33      -0.08   408    1
## bA    -1.97   0.24      -2.35      -1.59   521    1
## bAR    0.40   0.14       0.17       0.62   474    1
## sigma  0.95   0.06       0.86       1.03   760    1
</code></pre>

<p>It is possible to draw more samples from the stan model, also using more chains:</p>

<pre><code class="language-r">m8.1stan_4chains &lt;- map2stan( m8.1stan, chains=4, cores=4)
</code></pre>

<pre><code>## 
## SAMPLING FOR MODEL 'log_gdp ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 3.4e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.
## Adjust your expectations accordingly!
## 
## 
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## Iteration: 1 / 1 [100%]  (Sampling)
## 
##  Elapsed Time: 1e-06 seconds (Warm-up)
##                5.4e-05 seconds (Sampling)
##                5.5e-05 seconds (Total)

## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

## Warning: Examine the pairs() plot to diagnose sampling problems

## Computing WAIC

## Constructing posterior predictions

## [ 400 / 4000 ]
[ 800 / 4000 ]
[ 1200 / 4000 ]
[ 1600 / 4000 ]
[ 2000 / 4000 ]
[ 2400 / 4000 ]
[ 2800 / 4000 ]
[ 3200 / 4000 ]
[ 3600 / 4000 ]
[ 4000 / 4000 ]
</code></pre>

<pre><code class="language-r">precis(m8.1stan_4chains)
</code></pre>

<pre><code>##        Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
## a      9.22   0.14       9.00       9.44  1842    1
## bR    -0.20   0.08      -0.33      -0.09  1861    1
## bA    -1.94   0.23      -2.29      -1.57  1852    1
## bAR    0.39   0.13       0.19       0.61  2004    1
## sigma  0.95   0.05       0.87       1.04  2774    1
</code></pre>

<p>To visualize the results, you can plot the samples. To pull out samples, use</p>

<pre><code class="language-r">post &lt;- extract.samples( m8.1stan )
str(post)
</code></pre>

<pre><code>## List of 5
##  $ a    : num [1:1000(1d)] 9.12 8.88 9.45 9.43 9.2 ...
##  $ bR   : num [1:1000(1d)] -0.1544 -0.0302 -0.2736 -0.2696 -0.2287 ...
##  $ bA   : num [1:1000(1d)] -2.03 -1.35 -2.29 -2.16 -2.12 ...
##  $ bAR  : num [1:1000(1d)] 0.443 0.159 0.469 0.454 0.465 ...
##  $ sigma: num [1:1000(1d)] 0.897 0.898 1.009 0.974 0.997 ...
</code></pre>

<pre><code class="language-r">pairs(post)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-10-1.png" alt="" /></p>

<p>A prettier plot is also available, directly on the stan model:</p>

<pre><code class="language-r">pairs( m8.1stan )
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-11-1.png" alt="" /></p>

<p>By default, <code>map2stan</code> computes DIC and WAIC. We can extract them with</p>

<pre><code class="language-r">DIC(m8.1stan)
</code></pre>

<pre><code>## [1] 469.1977
## attr(,&quot;pD&quot;)
## [1] 5.156756
</code></pre>

<p>and</p>

<pre><code class="language-r">WAIC(m8.1stan)
</code></pre>

<pre><code>## [1] 469.6209
## attr(,&quot;lppd&quot;)
## [1] -229.6164
## attr(,&quot;pWAIC&quot;)
## [1] 5.194078
## attr(,&quot;se&quot;)
## [1] 14.84488
</code></pre>

<p>Alternatively, it is also displayed in the default <code>show</code> output:</p>

<pre><code class="language-r">show(m8.1stan)
</code></pre>

<pre><code>## map2stan model fit
## 1000 samples from 1 chain
## 
## Formula:
## log_gdp ~ dnorm(mu, sigma)
## mu &lt;- a + bR * rugged + bA * cont_africa + bAR * rugged * cont_africa
## a ~ dnorm(0, 100)
## bR ~ dnorm(0, 10)
## bA ~ dnorm(0, 10)
## bAR ~ dnorm(0, 10)
## sigma ~ dcauchy(0, 2)
## 
## Log-likelihood at expected values: -229.44 
## Deviance: 458.88 
## DIC: 469.2 
## Effective number of parameters (pD): 5.16 
## 
## WAIC (SE): 469.62 (14.8)
## pWAIC: 5.19
</code></pre>

<p>To get the trace plots of the Markov Chain:</p>

<pre><code class="language-r">plot( m8.1stan, window=c(100,2000), col=&quot;royalblue4&quot;, n_cols=2)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-15-1.png" alt="" /></p>

<p>To get a glimpse at the raw stan code, we can use <code>stancode()</code></p>

<pre><code class="language-r">stancode(m8.1stan)
</code></pre>

<pre><code>## data{
##     int&lt;lower=1&gt; N;
##     real log_gdp[N];
##     real rugged[N];
##     int cont_africa[N];
## }
## parameters{
##     real a;
##     real bR;
##     real bA;
##     real bAR;
##     real&lt;lower=0&gt; sigma;
## }
## model{
##     vector[N] mu;
##     sigma ~ cauchy( 0 , 2 );
##     bAR ~ normal( 0 , 10 );
##     bA ~ normal( 0 , 10 );
##     bR ~ normal( 0 , 10 );
##     a ~ normal( 0 , 100 );
##     for ( i in 1:N ) {
##         mu[i] = a + bR * rugged[i] + bA * cont_africa[i] + bAR * rugged[i] * cont_africa[i];
##     }
##     log_gdp ~ normal( mu , sigma );
## }
## generated quantities{
##     vector[N] mu;
##     real dev;
##     dev = 0;
##     for ( i in 1:N ) {
##         mu[i] = a + bR * rugged[i] + bA * cont_africa[i] + bAR * rugged[i] * cont_africa[i];
##     }
##     dev = dev + (-2)*normal_lpdf( log_gdp | mu , sigma );
## }
</code></pre>

<h2 id="8-4-care-and-feeding-of-your-markov-chain">8.4 Care and feeding of your Markov chain</h2>

<p>Example of non-convergent chain:</p>

<pre><code class="language-r">y &lt;- c(-1, 1)
m8.2 &lt;- map2stan(
  alist(
    y ~ dnorm( mu, sigma),
    mu &lt;- alpha
  ),
  data=list(y=y), start=list(alpha=0, sigma=1),
  chains=2, iter=4000, warmup=1000
)
</code></pre>

<pre><code>## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 4e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.043752 seconds (Warm-up)
##                0.530917 seconds (Sampling)
##                0.574669 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 2).
## 
## Gradient evaluation took 3e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.087006 seconds (Warm-up)
##                0.730778 seconds (Sampling)
##                0.817784 seconds (Total)

## Warning: There were 190 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

## Warning: There were 517 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded

## Warning: There were 2 chains where the estimated Bayesian Fraction of Missing Information was low. See
## http://mc-stan.org/misc/warnings.html#bfmi-low

## Warning: Examine the pairs() plot to diagnose sampling problems

## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 3e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Adjust your expectations accordingly!
## 
## 
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## Iteration: 1 / 1 [100%]  (Sampling)
## 
##  Elapsed Time: 1e-06 seconds (Warm-up)
##                1.6e-05 seconds (Sampling)
##                1.7e-05 seconds (Total)

## Computing WAIC

## Constructing posterior predictions

## [ 600 / 6000 ]
[ 1200 / 6000 ]
[ 1800 / 6000 ]
[ 2400 / 6000 ]
[ 3000 / 6000 ]
[ 3600 / 6000 ]
[ 4200 / 6000 ]
[ 4800 / 6000 ]
[ 5400 / 6000 ]
[ 6000 / 6000 ]

## Warning in map2stan(alist(y ~ dnorm(mu, sigma), mu &lt;- alpha), data = list(y = y), : There were 190 divergent iterations during sampling.
## Check the chains (trace plots, n_eff, Rhat) carefully to ensure they are valid.
</code></pre>

<p>There are quite a few warnings on divergencies. Let&rsquo;s have a look at the estimates:</p>

<pre><code class="language-r">precis(m8.2)
</code></pre>

<pre><code>## Warning in precis(m8.2): There were 190 divergent iterations during sampling.
## Check the chains (trace plots, n_eff, Rhat) carefully to ensure they are valid.

##            Mean     StdDev   lower 0.89 upper 0.89 n_eff Rhat
## alpha -14918610   45488810 -79876626.25   40109163    25  1.1
## sigma 226946981 3562046858      6040.97  199672504  1771  1.0
</code></pre>

<p>This doesn&rsquo;t look right: The estimates are a very far way out there, the effective number of samples is relatively low and <code>Rhat</code> is above 1. While <code>Rhat</code> in my case is only around 1.01, even such a value is already suspicious. Let&rsquo;s have a look at the trace plots.</p>

<pre><code class="language-r">plot(m8.2, col=c(&quot;black&quot;, &quot;royalblue4&quot;), n_cols=1)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-19-1.png" alt="" /></p>

<p>The problem: The priors are very flat which means that even values of 500 millions are plausible values. We can fix this by adding a weakly informative prior:</p>

<pre><code class="language-r">m8.3 &lt;- map2stan(
  alist(
    y ~ dnorm( mu, sigma),
    mu &lt;- alpha,
    alpha ~ dnorm(1, 10),
    sigma ~ dcauchy( 0, 1)
  ),
  data=list(y=y), start=list(alpha=0, sigma=1),
  chains=2, iter=4000, warmup=1000
)
</code></pre>

<pre><code>## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 1.3e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.014807 seconds (Warm-up)
##                0.040731 seconds (Sampling)
##                0.055538 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 2).
## 
## Gradient evaluation took 3e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.012045 seconds (Warm-up)
##                0.036529 seconds (Sampling)
##                0.048574 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 4e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Adjust your expectations accordingly!
## 
## 
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## Iteration: 1 / 1 [100%]  (Sampling)
## 
##  Elapsed Time: 0 seconds (Warm-up)
##                1.4e-05 seconds (Sampling)
##                1.4e-05 seconds (Total)

## Computing WAIC

## Constructing posterior predictions

## [ 600 / 6000 ]
[ 1200 / 6000 ]
[ 1800 / 6000 ]
[ 2400 / 6000 ]
[ 3000 / 6000 ]
[ 3600 / 6000 ]
[ 4200 / 6000 ]
[ 4800 / 6000 ]
[ 5400 / 6000 ]
[ 6000 / 6000 ]
</code></pre>

<pre><code class="language-r">precis(m8.3)
</code></pre>

<pre><code>##       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
## alpha 0.02   1.77      -2.54       2.25  1254    1
## sigma 2.16   2.22       0.46       3.84  1142    1
</code></pre>

<p>The estimates seem much more reasonable and the <code>Rhat</code> value is now 1.</p>

<pre><code class="language-r">plot(m8.3, col=c(&quot;black&quot;, &quot;royalblue4&quot;), n_cols=1)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-21-1.png" alt="" /></p>

<p>The chains also look good now.</p>

<p>If we compare the prior and posterior distribution, even two points can overcome these weakly informative priors and thus lead to better results than flat priors.</p>

<pre><code class="language-r">post &lt;- extract.samples(m8.3)
par(mfrow=c(1, 2))
sq &lt;- seq(-15, 20, length.out = 100)
plot( density(post$alpha,  from=-15, to=20, adj=1),
      lwd=2, col=&quot;royalblue4&quot;, xlab=&quot;alpha&quot;, 
     main=&quot;&quot;)
points(sq, dnorm(sq, 1, 10), type=&quot;l&quot;, lty=2)
text(4.5, 0.3, labels = &quot;Posterior&quot;)
text(8, 0.06, labels=&quot;Prior&quot;)

sq &lt;- seq(0, 10, length.out = 100)
plot( density( post$sigma, from=0, to=10, adj=1.5),
      lwd=2, col=&quot;royalblue4&quot;, xlab=&quot;sigma&quot;, 
      main=&quot;&quot;)
points(sq, 2*dcauchy(sq, 0, 1), type=&quot;l&quot;, lty=2)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-22-1.png" alt="" /></p>

<h3 id="non-identifiable-parameters">Non-identifiable parameters</h3>

<p>We&rsquo;ve learned before how highly correlated predictors lead to non-identifiable parameters. Let&rsquo;s have a look how these look inside a Markov chain.</p>

<pre><code class="language-r">y &lt;- rnorm( 100, mean=0, sd=1 )
</code></pre>

<p>We fit the following unidentifiable model:</p>

<pre><code class="language-r">m8.4 &lt;- map2stan(
  alist(
    y ~ dnorm( mu, sigma),
    mu &lt;- a1 + a2,
    sigma ~ dcauchy( 0, 1)
  ), 
  data=list(y=y), start=list(a1=0, a2=0, sigma=1),
  chains=2, iter=4000, warmup=1000
)
</code></pre>

<pre><code>## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 8e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 2.75859 seconds (Warm-up)
##                9.17401 seconds (Sampling)
##                11.9326 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 2).
## 
## Gradient evaluation took 5e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 2.66443 seconds (Warm-up)
##                8.63442 seconds (Sampling)
##                11.2988 seconds (Total)

## Warning: There were 4900 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded

## Warning: Examine the pairs() plot to diagnose sampling problems

## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 6e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Adjust your expectations accordingly!
## 
## 
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## Iteration: 1 / 1 [100%]  (Sampling)
## 
##  Elapsed Time: 1e-06 seconds (Warm-up)
##                2e-05 seconds (Sampling)
##                2.1e-05 seconds (Total)

## Computing WAIC

## Constructing posterior predictions

## [ 600 / 6000 ]
[ 1200 / 6000 ]
[ 1800 / 6000 ]
[ 2400 / 6000 ]
[ 3000 / 6000 ]
[ 3600 / 6000 ]
[ 4200 / 6000 ]
[ 4800 / 6000 ]
[ 5400 / 6000 ]
[ 6000 / 6000 ]
</code></pre>

<pre><code class="language-r">precis(m8.4)
</code></pre>

<pre><code>##           Mean  StdDev lower 0.89 upper 0.89 n_eff Rhat
## a1     1214.73 1803.43   -1269.33    3585.98     1 5.77
## a2    -1214.71 1803.43   -3585.96    1269.14     1 5.77
## sigma     1.14    0.08       1.01       1.26    25 1.05
</code></pre>

<p>These estimates of <code>a1</code> and <code>a2</code> look suspicious. Also, <code>n_eff</code> and <code>Rhat</code> have terrible values.</p>

<pre><code class="language-r">plot(m8.4, col=c(&quot;black&quot;, &quot;royalblue4&quot;), n_cols=1)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-25-1.png" alt="" /></p>

<p>The trace plots also don&rsquo;t look good: The two chains are not mixing and are definitely not stationary. Again, we can use weak priors to solve this problem:</p>

<pre><code class="language-r">m8.5 &lt;- map2stan(
  alist(
    y ~ dnorm( mu, sigma),
    mu &lt;- a1 + a2,
    a1 ~ dnorm(0, 10),
    a2 ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ),
  data=list(y=y), start=list(a1=0, a2=0, sigma=1),
  chains=2, iter=4000, warmup=1000
)
</code></pre>

<pre><code>## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 1.1e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.747197 seconds (Warm-up)
##                2.54276 seconds (Sampling)
##                3.28995 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 2).
## 
## Gradient evaluation took 6e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 4000 [  0%]  (Warmup)
## Iteration:  400 / 4000 [ 10%]  (Warmup)
## Iteration:  800 / 4000 [ 20%]  (Warmup)
## Iteration: 1001 / 4000 [ 25%]  (Sampling)
## Iteration: 1400 / 4000 [ 35%]  (Sampling)
## Iteration: 1800 / 4000 [ 45%]  (Sampling)
## Iteration: 2200 / 4000 [ 55%]  (Sampling)
## Iteration: 2600 / 4000 [ 65%]  (Sampling)
## Iteration: 3000 / 4000 [ 75%]  (Sampling)
## Iteration: 3400 / 4000 [ 85%]  (Sampling)
## Iteration: 3800 / 4000 [ 95%]  (Sampling)
## Iteration: 4000 / 4000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.684015 seconds (Warm-up)
##                2.67554 seconds (Sampling)
##                3.35956 seconds (Total)
## 
## 
## SAMPLING FOR MODEL 'y ~ dnorm(mu, sigma)' NOW (CHAIN 1).
## 
## Gradient evaluation took 6e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
## Adjust your expectations accordingly!
## 
## 
## WARNING: No variance estimation is
##          performed for num_warmup &lt; 20
## 
## Iteration: 1 / 1 [100%]  (Sampling)
## 
##  Elapsed Time: 1e-06 seconds (Warm-up)
##                2.3e-05 seconds (Sampling)
##                2.4e-05 seconds (Total)

## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup

## Warning: Examine the pairs() plot to diagnose sampling problems

## Computing WAIC

## Constructing posterior predictions

## [ 600 / 6000 ]
[ 1200 / 6000 ]
[ 1800 / 6000 ]
[ 2400 / 6000 ]
[ 3000 / 6000 ]
[ 3600 / 6000 ]
[ 4200 / 6000 ]
[ 4800 / 6000 ]
[ 5400 / 6000 ]
[ 6000 / 6000 ]
</code></pre>

<pre><code class="language-r">precis(m8.5)
</code></pre>

<pre><code>##        Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
## a1     0.34   7.02     -10.88      11.45  1496    1
## a2    -0.31   7.02     -11.31      11.09  1495    1
## sigma  1.13   0.08       1.01       1.26  1968    1
</code></pre>

<p>Not only did the model sample much faster, both the estimates and the values for <code>n_eff</code> and <code>Rhat</code> look much better.</p>

<pre><code class="language-r">plot(m8.5, col=c(&quot;black&quot;, &quot;royalblue4&quot;), n_cols=1)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-27-1.png" alt="" /></p>

<p>The trace plots as well look very good: stationary and well mixed.</p>

<h3 id="overthinking-cauchy-distribution">Overthinking: Cauchy distribution</h3>

<p>The Cauchy distribution does not have mean since it has a very thick-tailed distribution. At any moment in a Cauchy sampling process, a very high value can be drawn that overwhelms all of the previous draw and hence the the distribution does not converge to a mean.</p>

<pre><code class="language-r">set.seed(13)
y &lt;- rcauchy(1e4, 0, 5)
mu &lt;- sapply(1:length(y), function(i) sum(y[1:i]/i))
plot(mu, type=&quot;l&quot;)
</code></pre>

<p><img src="chapter8_files/figure-markdown_github/unnamed-chunk-28-1.png" alt="" /></p>

    
    </section>


  <aside class="read-next">
  
      <a class="read-next-story" style="no-cover" href="/projects/statistical-rethinking/chapter_7/chapter7_ex/">
          <section class="post">
              <h5></h5>
              
          </section>
      </a>
  
  
      <a class="read-next-story prev" style="no-cover" href="/projects/statistical-rethinking/chapter_8/chapter8_ex/">
          <section class="post">
              <h5></h5>
          </section>
      </a>
      

</aside>



  <footer class="post-footer">


    









<section class="author">
  <h4><a href="/">Corrie</a></h4>
  
  <p>Read <a href="/">more posts</a> by this author.</p>
  
  <div class="author-meta">
    
    
  </div>
</section>




    
<section class="share">
  <h4>Share this projects</h4>
  <a class="icon-twitter" style="font-size: 1.4em" href="https://twitter.com/share?text=&nbsp;-&nbsp;Samples%20of%20Thoughts&amp;url=%2fprojects%2fstatistical-rethinking%2fchapter_8%2fchapter8%2f"
      onclick="window.open(this.href, 'twitter-share', 'width=550,height=421');return false;">
      <span class="hidden">Twitter</span>
  </a>
  <a class="icon-facebook" style="font-size: 1.4em" href="https://www.facebook.com/sharer/sharer.php?u=%2fprojects%2fstatistical-rethinking%2fchapter_8%2fchapter8%2f"
      onclick="window.open(this.href, 'facebook-share','width=580,height=551');return false;">
      <span class="hidden">Facebook</span>
  </a>
  <a class="icon-linkedin" style="font-size: 1.4em" href="https://www.linkedin.com/shareArticle?mini=true&title=&url=%2fprojects%2fstatistical-rethinking%2fchapter_8%2fchapter8%2f"
               onclick="window.open(this.href, 'linkedin-share', 'width=554,height=571');return false;">
    <span class="hidden">LinkedIn</span>
    </a>

</section>




    

<div id="disqus_thread"></div>
<script>




var disqus_config = function () {
this.page.url = "\/projects\/statistical-rethinking\/chapter_8\/chapter8\/";  
this.page.identifier = "\/projects\/statistical-rethinking\/chapter_8\/chapter8\/"; 
};

(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://corriebar-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>








  </footer>
</article>

</main>



    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Samples of Thoughts</a> 
        &copy; Corrie Bartelheimer 2020 &middot; </section>
        
    </footer>
    </div>
    <script type="text/javascript" src="/js/jquery.js"></script>
    <script type="text/javascript" src="/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="/js/index.js"></script>
    
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>


    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
    </script>
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML">
</script>


    
    
<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-140745376-1', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>


    
</body>
</html>

